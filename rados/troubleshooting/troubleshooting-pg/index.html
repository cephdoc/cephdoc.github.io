
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>归置组排障 &mdash; Ceph Documentation</title>
    
    <link rel="stylesheet" href="../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     'dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="top" title="Ceph Documentation" href="../../../" />
    <link rel="up" title="故障排除" href="../" />
    <link rel="next" title="内存剖析" href="../memory-profiling/" />
    <link rel="prev" title="OSD 故障排除" href="../troubleshooting-osd/" />
    <script type="text/javascript" src="http://ayni.ceph.com/public/js/ceph.js"></script>

  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../memory-profiling/" title="内存剖析"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../troubleshooting-osd/" title="OSD 故障排除"
             accesskey="P">previous</a> |</li>
        <li><a href="../../../">Ceph Documentation</a> &raquo;</li>
          <li><a href="../../" >Ceph 存储集群</a> &raquo;</li>
          <li><a href="../" accesskey="U">故障排除</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="id1">
<h1>归置组排障<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id2">
<h2>归置组总不整洁<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>When you create a cluster and your cluster remains in <tt class="docutils literal"><span class="pre">active</span></tt>,
<tt class="docutils literal"><span class="pre">active+remapped</span></tt> or <tt class="docutils literal"><span class="pre">active+degraded</span></tt> status and never achieve an
<tt class="docutils literal"><span class="pre">active+clean</span></tt> status, you likely have a problem with your configuration.</p>
<p>You may need to review settings in the <a class="reference external" href="../../configuration/pool-pg-config-ref">存储池、归置组和 CRUSH 配置参考</a>
and make appropriate adjustments.</p>
<p>As a general rule, you should run your cluster with more than one OSD and a
pool size greater than 1 object replica.</p>
<div class="section" id="one-node-cluster">
<h3>One Node Cluster<a class="headerlink" href="#one-node-cluster" title="Permalink to this headline">¶</a></h3>
<p>Ceph no longer provides documentation for operating on a single node, because
you would never deploy a system designed for distributed computing on a single
node. Additionally, mounting client kernel modules on a single node containing a
Ceph  daemon may cause a deadlock due to issues with the Linux kernel itself
(unless you use VMs for the clients). You can experiment with Ceph in a 1-node
configuration, in spite of the limitations as described herein.</p>
<p>If you are trying to create a cluster on a single node, you must change the
default of the <tt class="docutils literal"><span class="pre">osd</span> <span class="pre">crush</span> <span class="pre">chooseleaf</span> <span class="pre">type</span></tt> setting from <tt class="docutils literal"><span class="pre">1</span></tt> (meaning
<tt class="docutils literal"><span class="pre">host</span></tt> or <tt class="docutils literal"><span class="pre">node</span></tt>) to <tt class="docutils literal"><span class="pre">0</span></tt> (meaning <tt class="docutils literal"><span class="pre">osd</span></tt>) in your Ceph configuration
file before you create your monitors and OSDs. This tells Ceph that an OSD
can peer with another OSD on the same host. If you are trying to set up a
1-node cluster and <tt class="docutils literal"><span class="pre">osd</span> <span class="pre">crush</span> <span class="pre">chooseleaf</span> <span class="pre">type</span></tt> is greater than <tt class="docutils literal"><span class="pre">0</span></tt>,
Ceph will try to peer the PGs of one OSD with the PGs of another OSD on
another node, chassis, rack, row, or even datacenter depending on the setting.</p>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">DO NOT mount kernel clients directly on the same node as your
Ceph Storage Cluster, because kernel conflicts can arise. However, you
can mount kernel clients within virtual machines (VMs) on a single node.</p>
</div>
<p>If you are creating OSDs using a single disk, you must create directories
for the data manually first. For example:</p>
<div class="highlight-python"><pre>mkdir /var/local/osd0 /var/local/osd1
ceph-deploy osd prepare {localhost-name}:/var/local/osd0 {localhost-name}:/var/local/osd1
ceph-deploy osd activate {localhost-name}:/var/local/osd0 {localhost-name}:/var/local/osd1</pre>
</div>
</div>
<div class="section" id="fewer-osds-than-replicas">
<h3>Fewer OSDs than Replicas<a class="headerlink" href="#fewer-osds-than-replicas" title="Permalink to this headline">¶</a></h3>
<p>If you&#8217;ve brought up two OSDs to an <tt class="docutils literal"><span class="pre">up</span></tt> and <tt class="docutils literal"><span class="pre">in</span></tt> state, but you still
don&#8217;t see <tt class="docutils literal"><span class="pre">active</span> <span class="pre">+</span> <span class="pre">clean</span></tt> placement groups, you may have an
<tt class="docutils literal"><span class="pre">osd</span> <span class="pre">pool</span> <span class="pre">default</span> <span class="pre">size</span></tt> set to greater than <tt class="docutils literal"><span class="pre">2</span></tt>.</p>
<p>There are a few ways to address this situation. If you want to operate your
cluster in an <tt class="docutils literal"><span class="pre">active</span> <span class="pre">+</span> <span class="pre">degraded</span></tt> state with two replicas, you can set the
<tt class="docutils literal"><span class="pre">osd</span> <span class="pre">pool</span> <span class="pre">default</span> <span class="pre">min</span> <span class="pre">size</span></tt> to <tt class="docutils literal"><span class="pre">2</span></tt> so that you can write objects in
an <tt class="docutils literal"><span class="pre">active</span> <span class="pre">+</span> <span class="pre">degraded</span></tt> state. You may also set the <tt class="docutils literal"><span class="pre">osd</span> <span class="pre">pool</span> <span class="pre">default</span> <span class="pre">size</span></tt>
setting to <tt class="docutils literal"><span class="pre">2</span></tt> so that you only have two stored replicas (the original and
one replica), in which case the cluster should achieve an <tt class="docutils literal"><span class="pre">active</span> <span class="pre">+</span> <span class="pre">clean</span></tt>
state.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can make the changes at runtime. If you make the changes in
your Ceph configuration file, you may need to restart your cluster.</p>
</div>
</div>
<div class="section" id="pool-size-1">
<h3>Pool Size = 1<a class="headerlink" href="#pool-size-1" title="Permalink to this headline">¶</a></h3>
<p>If you have the <tt class="docutils literal"><span class="pre">osd</span> <span class="pre">pool</span> <span class="pre">default</span> <span class="pre">size</span></tt> set to <tt class="docutils literal"><span class="pre">1</span></tt>, you will only have
one copy of the object. OSDs rely on other OSDs to tell them which objects
they should have. If a first OSD has a copy of an object and there is no
second copy, then no second OSD can tell the first OSD that it should have
that copy. For each placement group mapped to the first OSD (see
<tt class="docutils literal"><span class="pre">ceph</span> <span class="pre">pg</span> <span class="pre">dump</span></tt>), you can force the first OSD to notice the placement groups
it needs by running:</p>
<div class="highlight-python"><pre>ceph pg force_create_pg &lt;pgid&gt;</pre>
</div>
</div>
<div class="section" id="crush">
<h3>CRUSH 图错误<a class="headerlink" href="#crush" title="Permalink to this headline">¶</a></h3>
<p>Another candidate for placement groups remaining unclean involves errors
in your CRUSH map.</p>
</div>
</div>
<div class="section" id="id3">
<h2>卡住的归置组<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>有失败时归置组会进入“degraded”（降级）或“peering”（连接建立中）状态，这事时有发生，通常这些状态意味着正常的失败恢复正在进行。然而，如果一个归置组长时间处于某个这些状态就意味着有更大的问题，因此监视器在归置组卡 （stuck） 在非最优状态时会警告。我们具体检查：</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">inactive</span></tt> （不活跃）——归置组长时间无活跃（即它不能提供读写服务了）；</li>
<li><tt class="docutils literal"><span class="pre">unclean</span></tt> （不干净）——归置组长时间不干净（例如它未能从前面的失败完全恢复）；</li>
<li><tt class="docutils literal"><span class="pre">stale</span></tt> （不新鲜）——归置组状态没有被 <tt class="docutils literal"><span class="pre">ceph-osd</span></tt> 更新，表明存储这个归置组的所有节点可能都挂了。</li>
</ul>
<p>你可以摆出卡住的归置组：</p>
<div class="highlight-python"><pre>ceph pg dump_stuck stale
ceph pg dump_stuck inactive
ceph pg dump_stuck unclean</pre>
</div>
<p>卡在 <tt class="docutils literal"><span class="pre">stale</span></tt> 状态的归置组通过修复 <tt class="docutils literal"><span class="pre">ceph-osd</span></tt> 进程通常可以修复；卡在 <tt class="docutils literal"><span class="pre">inactive</span></tt> 状态的归置组通常是互联问题（参见 <a class="reference internal" href="#failures-osd-peering"><em>归置组挂了——互联失败</em></a> ）；卡在 <tt class="docutils literal"><span class="pre">unclean</span></tt> 状态的归置组通常是由于某些原因阻止了恢复的完成，像未找到的对象（参见 <a class="reference internal" href="#failures-osd-unfound"><em>未找到的对象</em></a> ）。</p>
</div>
<div class="section" id="failures-osd-peering">
<span id="id4"></span><h2>归置组挂了——互联失败<a class="headerlink" href="#failures-osd-peering" title="Permalink to this headline">¶</a></h2>
<p>在某些情况下， <tt class="docutils literal"><span class="pre">ceph-osd</span></tt> 连接建立进程会遇到问题，使 PG 不能活跃、可用，例如 <tt class="docutils literal"><span class="pre">ceph</span> <span class="pre">health</span></tt> 也许显示：</p>
<div class="highlight-python"><pre>ceph health detail
HEALTH_ERR 7 pgs degraded; 12 pgs down; 12 pgs peering; 1 pgs recovering; 6 pgs stuck unclean; 114/3300 degraded (3.455%); 1/3 in osds are down
...
pg 0.5 is down+peering
pg 1.4 is down+peering
...
osd.1 is down since epoch 69, last address 192.168.106.220:6801/8651</pre>
</div>
<p>可以查询到 PG 为何被标记为 <tt class="docutils literal"><span class="pre">down</span></tt> ：</p>
<div class="highlight-python"><pre>ceph pg 0.5 query</pre>
</div>
<div class="highlight-javascript"><div class="highlight"><pre><span class="p">{</span> <span class="s2">&quot;state&quot;</span><span class="o">:</span> <span class="s2">&quot;down+peering&quot;</span><span class="p">,</span>
  <span class="p">...</span>
  <span class="s2">&quot;recovery_state&quot;</span><span class="o">:</span> <span class="p">[</span>
       <span class="p">{</span> <span class="s2">&quot;name&quot;</span><span class="o">:</span> <span class="s2">&quot;Started\/Primary\/Peering\/GetInfo&quot;</span><span class="p">,</span>
         <span class="s2">&quot;enter_time&quot;</span><span class="o">:</span> <span class="s2">&quot;2012-03-06 14:40:16.169679&quot;</span><span class="p">,</span>
         <span class="s2">&quot;requested_info_from&quot;</span><span class="o">:</span> <span class="p">[]},</span>
       <span class="p">{</span> <span class="s2">&quot;name&quot;</span><span class="o">:</span> <span class="s2">&quot;Started\/Primary\/Peering&quot;</span><span class="p">,</span>
         <span class="s2">&quot;enter_time&quot;</span><span class="o">:</span> <span class="s2">&quot;2012-03-06 14:40:16.169659&quot;</span><span class="p">,</span>
         <span class="s2">&quot;probing_osds&quot;</span><span class="o">:</span> <span class="p">[</span>
               <span class="mi">0</span><span class="p">,</span>
               <span class="mi">1</span><span class="p">],</span>
         <span class="s2">&quot;blocked&quot;</span><span class="o">:</span> <span class="s2">&quot;peering is blocked due to down osds&quot;</span><span class="p">,</span>
         <span class="s2">&quot;down_osds_we_would_probe&quot;</span><span class="o">:</span> <span class="p">[</span>
               <span class="mi">1</span><span class="p">],</span>
         <span class="s2">&quot;peering_blocked_by&quot;</span><span class="o">:</span> <span class="p">[</span>
               <span class="p">{</span> <span class="s2">&quot;osd&quot;</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="s2">&quot;current_lost_at&quot;</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="s2">&quot;comment&quot;</span><span class="o">:</span> <span class="s2">&quot;starting or marking this osd lost may let us proceed&quot;</span><span class="p">}]},</span>
       <span class="p">{</span> <span class="s2">&quot;name&quot;</span><span class="o">:</span> <span class="s2">&quot;Started&quot;</span><span class="p">,</span>
         <span class="s2">&quot;enter_time&quot;</span><span class="o">:</span> <span class="s2">&quot;2012-03-06 14:40:16.169513&quot;</span><span class="p">}</span>
   <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">recovery_state</span></tt> 段告诉我们连接建立因 <tt class="docutils literal"><span class="pre">ceph-osd</span></tt> 进程挂了而被阻塞，本例是 <tt class="docutils literal"><span class="pre">osd.1</span></tt> 挂了，启动这个进程应该就可以恢复。</p>
<p>另外，如果 <tt class="docutils literal"><span class="pre">osd.1</span></tt> 是灾难性的失败（如硬盘损坏），我们可以告诉集群它丢失（ <tt class="docutils literal"><span class="pre">lost</span></tt> ）了，让集群尽力完成副本拷贝。</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">集群不能保证其它数据副本是一致且最新就危险了！</p>
</div>
<p>让 Ceph 无论如何都继续：</p>
<div class="highlight-python"><pre>ceph osd lost 1</pre>
</div>
<p>恢复将继续。</p>
</div>
<div class="section" id="failures-osd-unfound">
<span id="id5"></span><h2>未找到的对象<a class="headerlink" href="#failures-osd-unfound" title="Permalink to this headline">¶</a></h2>
<p>某几种失败相组合可能导致 Ceph 抱怨有找不到（ <tt class="docutils literal"><span class="pre">unfound</span></tt> ）的对象：</p>
<div class="highlight-python"><pre>ceph health detail
HEALTH_WARN 1 pgs degraded; 78/3778 unfound (2.065%)
pg 2.4 is active+degraded, 78 unfound</pre>
</div>
<p>这意味着存储集群知道一些对象（或者存在对象的较新副本）存在，却没有找到它们的副本。下例展示了这种情况是如何发生的，一个 PG 的数据存储在 ceph-osd 1 和 2 上：</p>
<ul class="simple">
<li>1 挂了；</li>
<li>2 独自处理一些写动作；</li>
<li>1 起来了；</li>
<li>1 和 2 重新互联， 1 上面丢失的对象加入队列准备恢复；</li>
<li>新对象还未拷贝完， 2 挂了。</li>
</ul>
<p>这时， 1 知道这些对象存在，但是活着的 <tt class="docutils literal"><span class="pre">ceph-osd</span></tt> 都没有副本，这种情况下，读写这些对象的 IO 就会被阻塞，集群只能指望节点早点恢复。这时我们假设用户希望先得到一个 IO 错误。</p>
<p>首先，你应该确认哪些对象找不到了：</p>
<div class="highlight-python"><pre>ceph pg 2.4 list_missing [starting offset, in json]</pre>
</div>
<div class="highlight-javascript"><div class="highlight"><pre><span class="p">{</span> <span class="s2">&quot;offset&quot;</span><span class="o">:</span> <span class="p">{</span> <span class="s2">&quot;oid&quot;</span><span class="o">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
     <span class="s2">&quot;key&quot;</span><span class="o">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
     <span class="s2">&quot;snapid&quot;</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span>
     <span class="s2">&quot;hash&quot;</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span>
     <span class="s2">&quot;max&quot;</span><span class="o">:</span> <span class="mi">0</span><span class="p">},</span>
 <span class="s2">&quot;num_missing&quot;</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span>
 <span class="s2">&quot;num_unfound&quot;</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span>
 <span class="s2">&quot;objects&quot;</span><span class="o">:</span> <span class="p">[</span>
    <span class="p">{</span> <span class="s2">&quot;oid&quot;</span><span class="o">:</span> <span class="s2">&quot;object 1&quot;</span><span class="p">,</span>
      <span class="s2">&quot;key&quot;</span><span class="o">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
      <span class="s2">&quot;hash&quot;</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span>
      <span class="s2">&quot;max&quot;</span><span class="o">:</span> <span class="mi">0</span> <span class="p">},</span>
    <span class="p">...</span>
 <span class="p">],</span>
 <span class="s2">&quot;more&quot;</span><span class="o">:</span> <span class="mi">0</span><span class="p">}</span>
</pre></div>
</div>
<p>如果在一次查询里列出的对象太多， <tt class="docutils literal"><span class="pre">more</span></tt> 这个字段将为 <tt class="docutils literal"><span class="pre">true</span></tt> ，因此你可以查询更多。（命令行工具可能隐藏了，但这里没有）</p>
<p>其次，你可以找出哪些 OSD 上探测到、或可能包含数据：</p>
<div class="highlight-python"><pre>ceph pg 2.4 query</pre>
</div>
<div class="highlight-javascript"><div class="highlight"><pre><span class="s2">&quot;recovery_state&quot;</span><span class="o">:</span> <span class="p">[</span>
     <span class="p">{</span> <span class="s2">&quot;name&quot;</span><span class="o">:</span> <span class="s2">&quot;Started\/Primary\/Active&quot;</span><span class="p">,</span>
       <span class="s2">&quot;enter_time&quot;</span><span class="o">:</span> <span class="s2">&quot;2012-03-06 15:15:46.713212&quot;</span><span class="p">,</span>
       <span class="s2">&quot;might_have_unfound&quot;</span><span class="o">:</span> <span class="p">[</span>
             <span class="p">{</span> <span class="s2">&quot;osd&quot;</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span>
               <span class="s2">&quot;status&quot;</span><span class="o">:</span> <span class="s2">&quot;osd is down&quot;</span><span class="p">}]},</span>
</pre></div>
</div>
<p>本例中，集群知道 <tt class="docutils literal"><span class="pre">osd.1</span></tt> 可能有数据，但它挂了（ <tt class="docutils literal"><span class="pre">down</span></tt> ）。所有可能的状态有：</p>
<ul class="simple">
<li>已经探测到了</li>
<li>在查询</li>
<li>OSD 挂了</li>
<li>尚未查询</li>
</ul>
<p>有时候集群要花一些时间来查询可能的位置。</p>
<p>还有一种可能性，对象存在于其它位置却未被列出，例如，集群里的一个 <tt class="docutils literal"><span class="pre">ceph-osd</span></tt> 停止且被剔出，然后完全恢复了；后来的失败、恢复后仍有未找到的对象，它也不会觉得早已死亡的 <tt class="docutils literal"><span class="pre">ceph-osd</span></tt> 上仍可能包含这些对象。（这种情况几乎不太可能发生）。</p>
<p>如果所有位置都查询过了仍有对象丢失，那就得放弃丢失的对象了。这仍可能是罕见的失败组合导致的，集群在写入完成前，未能得知写入是否已执行。以下命令把未找到的（ unfound ）对象标记为丢失（ lost ）。</p>
<div class="highlight-python"><pre>ceph pg 2.5 mark_unfound_lost revert|delete</pre>
</div>
<p>上述最后一个参数告诉集群应如何处理丢失的对象。</p>
<p>delete 选项将导致完全删除它们。</p>
<p>revert 选项（纠删码存储池不可用）会回滚到前一个版本或者（如果它是新对象的话）删除它。要慎用，它可能迷惑那些期望对象存在的应用程序。</p>
</div>
<div class="section" id="id6">
<h2>无根归置组<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>拥有归置组拷贝的 OSD 都可以失败，在这种情况下，那一部分的对象存储不可用，监视器就不会收到那些归置组的状态更新了。为检测这种情况，监视器把任何主 OSD 失败的归置组标记为 <tt class="docutils literal"><span class="pre">stale</span></tt> （不新鲜），例如：</p>
<div class="highlight-python"><pre>ceph health
HEALTH_WARN 24 pgs stale; 3/300 in osds are down</pre>
</div>
<p>你能找出哪些归置组 <tt class="docutils literal"><span class="pre">stale</span></tt> 、和存储这些归置组的最新 OSD ，命令如下：</p>
<div class="highlight-python"><pre>ceph health detail
HEALTH_WARN 24 pgs stale; 3/300 in osds are down
...
pg 2.5 is stuck stale+active+remapped, last acting [2,0]
...
osd.10 is down since epoch 23, last address 192.168.106.220:6800/11080
osd.11 is down since epoch 13, last address 192.168.106.220:6803/11539
osd.12 is down since epoch 24, last address 192.168.106.220:6806/11861</pre>
</div>
<p>如果想使归置组 2.5 重新在线，例如，上面的输出告诉我们它最后由 <tt class="docutils literal"><span class="pre">osd.0</span></tt> 和 <tt class="docutils literal"><span class="pre">osd.2</span></tt> 处理，重启这些 <tt class="docutils literal"><span class="pre">ceph-osd</span></tt> 将恢复之（还有其它的很多 PG ）。</p>
</div>
<div class="section" id="osd">
<h2>只有几个 OSD 接收数据<a class="headerlink" href="#osd" title="Permalink to this headline">¶</a></h2>
<p>如果你的集群有很多节点，但只有其中几个接收数据，<a class="reference external" href="../../operations/placement-groups#get-the-number-of-placement-groups">检查</a>下存储池里的归置组数量。因为归置组是映射到多个 OSD 的，这样少量的归置组将不能分布于整个集群。试着创建个新存储池，其归置组数量是 OSD 数量的若干倍。详情见<a class="reference external" href="../../operations/placement-groups">归置组</a>，存储池的默认归置组数量没多大用，你可以参考<a class="reference external" href="../../configuration/pool-pg-config-ref">这里</a>更改它。</p>
</div>
<div class="section" id="id7">
<h2>不能写入数据<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>如果你的集群已启动，但一些 OSD 没起来，导致不能写入数据，确认下运行的 OSD 数量满足归置组要求的最低 OSD 数。如果不能满足， Ceph 就不会允许你写入数据，因为 Ceph 不能保证复制能如愿进行。详情参见<a class="reference external" href="../../configuration/pool-pg-config-ref">存储池、归置组和 CRUSH 配置参考</a>里的 <tt class="docutils literal"><span class="pre">osd</span> <span class="pre">pool</span> <span class="pre">default</span> <span class="pre">min</span> <span class="pre">size</span></tt> 。</p>
</div>
<div class="section" id="id8">
<h2>归置组不一致<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>If you receive an <tt class="docutils literal"><span class="pre">active</span> <span class="pre">+</span> <span class="pre">clean</span> <span class="pre">+</span> <span class="pre">inconsistent</span></tt> state, this may happen
due to an error during scrubbing. If the inconsistency is due to disk errors,
check your disks.</p>
<p>You can repair the inconsistent placement group by executing:</p>
<div class="highlight-python"><pre>ceph pg repair {placement-group-ID}</pre>
</div>
<p>If you receive <tt class="docutils literal"><span class="pre">active</span> <span class="pre">+</span> <span class="pre">clean</span> <span class="pre">+</span> <span class="pre">inconsistent</span></tt> states periodically due to
clock skew, you may consider configuring your <a class="reference external" href="http://en.wikipedia.org/wiki/Network_Time_Protocol">NTP</a> daemons on your
monitor hosts to act as peers. See <a class="reference external" href="http://www.ntp.org/">网络时间协议</a> and Ceph
<a class="reference external" href="../../configuration/mon-config-ref/#clock">时钟选项</a> for additional details.</p>
</div>
<div class="section" id="active-clean">
<h2>纠删编码的归置组不是 active+clean<a class="headerlink" href="#active-clean" title="Permalink to this headline">¶</a></h2>
<p>CRUSH 找不到足够多的 OSD 映射到某个 PG 时，它会显示为 <tt class="docutils literal"><span class="pre">2147483647</span></tt> ，意思是 ITEM_NONE 或 <tt class="docutils literal"><span class="pre">no</span> <span class="pre">OSD</span> <span class="pre">found</span></tt> ，例如：</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">2147483647</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
</pre></div>
</div>
<div class="section" id="id9">
<h3>OSD 不够多<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>如果 Ceph 集群仅有 8 个 OSD ，但是纠删码存储池需要 9 个，就会显示上面的错误。这时候，你仍然可以另外创建需要较少 OSD 的纠删码存储池：</p>
<div class="highlight-python"><pre>ceph osd erasure-code-profile set myprofile k=5 m=3
ceph osd pool create erasurepool 16 16 erasure myprofile</pre>
</div>
<p>或者新增一个 OSD ，这个 PG 会自动用上的。</p>
</div>
<div class="section" id="id10">
<h3>CRUSH 条件不能满足<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>即使集群拥有足够多的 OSD ， CRUSH 规则集的强制要求仍有可能无法满足。假如有 10 个 OSD 分布于两个主机上，且 CRUSH 规则集要求相同归置组不得使用位于同一主机的两个 OSD ，这样映射就会失败，因为只能找到两个 OSD ，你可以从规则集里查看必要条件：</p>
<div class="highlight-python"><pre>$ ceph osd crush rule ls
[
    "replicated_ruleset",
    "erasurepool"]
$ ceph osd crush rule dump erasurepool
{ "rule_id": 1,
  "rule_name": "erasurepool",
  "ruleset": 1,
  "type": 3,
  "min_size": 3,
  "max_size": 20,
  "steps": [
        { "op": "take",
          "item": -1,
          "item_name": "default"},
        { "op": "chooseleaf_indep",
          "num": 0,
          "type": "host"},
        { "op": "emit"}]}</pre>
</div>
<p>可以这样解决此问题，创建新存储池，其内的 PG 允许多个 OSD 位于同一主机，命令如下：</p>
<div class="highlight-python"><pre>ceph osd erasure-code-profile set myprofile ruleset-failure-domain=osd
ceph osd pool create erasurepool 16 16 erasure myprofile</pre>
</div>
</div>
<div class="section" id="id11">
<h3>CRUSH 过早中止<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>假设集群拥有的 OSD 足以映射到 PG （比如有 9 个 OSD 和一个纠删码存储池的集群，每个 PG 需要 9 个 OSD ）， CRUSH 仍然有可能在找到映射前就中止了。可以这样解决：</p>
<ul class="simple">
<li>降低纠删存储池内 PG 的要求，让它使用较少的 OSD （需创建另一个存储池，因为纠删码配置不支持动态修改）。</li>
<li>向集群添加更多 OSD （无需修改纠删存储池，它会自动回到清洁状态）。</li>
<li>通过手工打造的 CRUSH 规则集，让它多试几次以找到合适的映射。把 <tt class="docutils literal"><span class="pre">set_choose_tries</span></tt> 设置得高于默认值即可。</li>
</ul>
<p>你从集群中提取出 crushmap 之后，应该先用 <tt class="docutils literal"><span class="pre">crushtool</span></tt> 校验一下是否有问题，这样你的试验就无需触及 Ceph 集群，只要在一个本地文件上测试即可：</p>
<div class="highlight-python"><pre>$ ceph osd crush rule dump erasurepool
{ "rule_name": "erasurepool",
  "ruleset": 1,
  "type": 3,
  "min_size": 3,
  "max_size": 20,
  "steps": [
        { "op": "take",
          "item": -1,
          "item_name": "default"},
        { "op": "chooseleaf_indep",
          "num": 0,
          "type": "host"},
        { "op": "emit"}]}
$ ceph osd getcrushmap &gt; crush.map
got crush map from osdmap epoch 13
$ crushtool -i crush.map --test --show-bad-mappings \
   --rule 1 \
   --num-rep 9 \
   --min-x 1 --max-x $((1024 * 1024))
bad mapping rule 8 x 43 num_rep 9 result [3,2,7,1,2147483647,8,5,6,0]
bad mapping rule 8 x 79 num_rep 9 result [6,0,2,1,4,7,2147483647,5,8]
bad mapping rule 8 x 173 num_rep 9 result [0,4,6,8,2,1,3,7,2147483647]</pre>
</div>
<p>其中 <tt class="docutils literal"><span class="pre">--num-rep</span></tt> 是纠删码 crush 规则集所需的 OSD 数量， <tt class="docutils literal"><span class="pre">--rule</span></tt> 是 <tt class="docutils literal"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">crush</span> <span class="pre">rule</span> <span class="pre">dump</span></tt> 命令结果中 <tt class="docutils literal"><span class="pre">ruleset</span></tt> 字段的值。此测试会尝试映射一百万个值（即 <tt class="docutils literal"><span class="pre">[--min-x,--max-x]</span></tt> 所指定的范围），且必须至少显示一个坏映射；如果它没有任何输出，说明所有映射都成功了，你可以就此打住：问题的根源不在这里。</p>
<p>反编译 crush 图后，你可以手动编辑其规则集：</p>
<div class="highlight-python"><pre>$ crushtool --decompile crush.map &gt; crush.txt</pre>
</div>
<p>并把下面这行加进规则集：</p>
<div class="highlight-python"><pre>step set_choose_tries 100</pre>
</div>
<p>然后 <tt class="docutils literal"><span class="pre">crush.txt</span></tt> 文件内的这部分大致如此：</p>
<div class="highlight-python"><pre>rule erasurepool {
        ruleset 1
        type erasure
        min_size 3
        max_size 20
        step set_chooseleaf_tries 5
        step set_choose_tries 100
        step take default
        step chooseleaf indep 0 type host
        step emit
}</pre>
</div>
<p>然后编译、并再次测试：</p>
<div class="highlight-python"><pre>$ crushtool --compile crush.txt -o better-crush.map</pre>
</div>
<p>所有映射都成功时，用 <tt class="docutils literal"><span class="pre">crushtool</span></tt> 的 <tt class="docutils literal"><span class="pre">--show-choose-tries</span></tt> 选项能看到成功映射的尝试次数直方图：</p>
<div class="highlight-python"><pre>$ crushtool -i better-crush.map --test --show-bad-mappings \
   --show-choose-tries \
   --rule 1 \
   --num-rep 9 \
   --min-x 1 --max-x $((1024 * 1024))
...
11:        42
12:        44
13:        54
14:        45
15:        35
16:        34
17:        30
18:        25
19:        19
20:        22
21:        20
22:        17
23:        13
24:        16
25:        13
26:        11
27:        11
28:        13
29:        11
30:        10
31:         6
32:         5
33:        10
34:         3
35:         7
36:         5
37:         2
38:         5
39:         5
40:         2
41:         5
42:         4
43:         1
44:         2
45:         2
46:         3
47:         1
48:         0
...
102:         0
103:         1
104:         0
...</pre>
</div>
<p>有 42 个归置组需 11 次重试、 44 个归置组需 12 次重试，以此类推。这样，重试的最高次数就是防止坏映射的最低值，也就是 <tt class="docutils literal"><span class="pre">set_choose_tries</span></tt> 的取值（即上面输出中的 103 ，因为任意归置组成功映射的重试次数都没有超过 103 ）。</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../">
              <img class="logo" src="../../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../../">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../start/intro/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../start/">安装（快速）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/">安装（手动）</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../">Ceph 存储集群</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../configuration/">配置</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../deployment/">部署</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../operations/">运维</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../operations/operating/">操纵集群</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/monitoring/">监控集群</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/monitoring-osd-pg/">监控 OSD 和归置组</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/user-management/">用户管理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/data-placement/">数据归置概览</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/pools/">存储池</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/erasure-code/">纠删码</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/cache-tiering/">分级缓存</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/placement-groups/">归置组</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/crush-map/">CRUSH 图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/add-or-rm-osds/">增加/删除 OSD</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/add-or-rm-mons/">增加/删除监视器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/control/">命令参考</a></li>
<li class="toctree-l3"><a class="reference internal" href="../community/">Ceph 社区</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting-mon/">监视器故障排除</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting-osd/">OSD 故障排除</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="">归置组排障</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id2">归置组总不整洁</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#one-node-cluster">One Node Cluster</a></li>
<li class="toctree-l5"><a class="reference internal" href="#fewer-osds-than-replicas">Fewer OSDs than Replicas</a></li>
<li class="toctree-l5"><a class="reference internal" href="#pool-size-1">Pool Size = 1</a></li>
<li class="toctree-l5"><a class="reference internal" href="#crush">CRUSH 图错误</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#id3">卡住的归置组</a></li>
<li class="toctree-l4"><a class="reference internal" href="#failures-osd-peering">归置组挂了——互联失败</a></li>
<li class="toctree-l4"><a class="reference internal" href="#failures-osd-unfound">未找到的对象</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">无根归置组</a></li>
<li class="toctree-l4"><a class="reference internal" href="#osd">只有几个 OSD 接收数据</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">不能写入数据</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">归置组不一致</a></li>
<li class="toctree-l4"><a class="reference internal" href="#active-clean">纠删编码的归置组不是 active+clean</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#id9">OSD 不够多</a></li>
<li class="toctree-l5"><a class="reference internal" href="#id10">CRUSH 条件不能满足</a></li>
<li class="toctree-l5"><a class="reference internal" href="#id11">CRUSH 过早中止</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../log-and-debug/">日志记录和调试</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cpu-profiling/">CPU 剖析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../memory-profiling/">内存剖析</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../man/">手册页</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../">故障排除</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../community/">Ceph 社区</a></li>
<li class="toctree-l3"><a class="reference internal" href="../log-and-debug/">日志记录和调试</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting-mon/">监视器故障排除</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting-osd/">OSD 故障排除</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="">归置组排障</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id2">归置组总不整洁</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#one-node-cluster">One Node Cluster</a></li>
<li class="toctree-l5"><a class="reference internal" href="#fewer-osds-than-replicas">Fewer OSDs than Replicas</a></li>
<li class="toctree-l5"><a class="reference internal" href="#pool-size-1">Pool Size = 1</a></li>
<li class="toctree-l5"><a class="reference internal" href="#crush">CRUSH 图错误</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#id3">卡住的归置组</a></li>
<li class="toctree-l4"><a class="reference internal" href="#failures-osd-peering">归置组挂了——互联失败</a></li>
<li class="toctree-l4"><a class="reference internal" href="#failures-osd-unfound">未找到的对象</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">无根归置组</a></li>
<li class="toctree-l4"><a class="reference internal" href="#osd">只有几个 OSD 接收数据</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">不能写入数据</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">归置组不一致</a></li>
<li class="toctree-l4"><a class="reference internal" href="#active-clean">纠删编码的归置组不是 active+clean</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#id9">OSD 不够多</a></li>
<li class="toctree-l5"><a class="reference internal" href="#id10">CRUSH 条件不能满足</a></li>
<li class="toctree-l5"><a class="reference internal" href="#id11">CRUSH 过早中止</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../memory-profiling/">内存剖析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cpu-profiling/">CPU 剖析</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api/">APIs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rbd/rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/">开发文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../release-notes/">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/">Ceph 版本</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../glossary/">Ceph 术语</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../memory-profiling/" title="内存剖析"
             >next</a> |</li>
        <li class="right" >
          <a href="../troubleshooting-osd/" title="OSD 故障排除"
             >previous</a> |</li>
        <li><a href="../../../">Ceph Documentation</a> &raquo;</li>
          <li><a href="../../" >Ceph 存储集群</a> &raquo;</li>
          <li><a href="../" >故障排除</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2010-2014, Inktank Storage, Inc. and contributors. Licensed under Creative Commons BY-SA.
    </div>
  </body>
</html>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>块设备与 OpenStack &mdash; Ceph Documentation</title>
    
    <link rel="stylesheet" href="../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     'dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="top" title="Ceph Documentation" href="../../" />
    <link rel="up" title="Ceph 块设备" href="../rbd/" />
    <link rel="next" title="块设备与 CloudStack" href="../rbd-cloudstack/" />
    <link rel="prev" title="librbd 选项" href="../rbd-config-ref/" />
    <script type="text/javascript" src="http://ayni.ceph.com/public/js/ceph.js"></script>

  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../rbd-cloudstack/" title="块设备与 CloudStack"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../rbd-config-ref/" title="librbd 选项"
             accesskey="P">previous</a> |</li>
        <li><a href="../../">Ceph Documentation</a> &raquo;</li>
          <li><a href="../rbd/" accesskey="U">Ceph 块设备</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="openstack">
<h1>块设备与 OpenStack<a class="headerlink" href="#openstack" title="Permalink to this headline">¶</a></h1>
<p id="index-0">通过 <tt class="docutils literal"><span class="pre">libvirt</span></tt> 你可以把 Ceph 块设备用于 OpenStack ，它配置了 QEMU 到 <tt class="docutils literal"><span class="pre">librbd</span></tt> 的接口。 Ceph 把块设备分块为对象并分布到集群中，这意味着大个的 Ceph 块设备映像其性能会比独立服务器更好。</p>
<p>要把 Ceph 块设备用于 OpenStack ，必须先安装 QEMU 、 <tt class="docutils literal"><span class="pre">libvirt</span></tt> 和 OpenStack 。我们建议用一台独立的物理主机安装 OpenStack ，此主机最少需 8GB 内存和一个 4 核 CPU 。下面的图表描述了 OpenStack/Ceph 技术栈。</p>
<p class="ditaa">
<img src="../../_images/ditaa-e4a4957f90e4d8ebac2608e1544c34bf784cfdfb.png"/>
</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">要让 OpenStack 使用 Ceph 块设备，你必须有相应的 Ceph 集群访问权限。</p>
</div>
<p>OpenStack 里有三个地方和 Ceph 块设备结合：</p>
<ul class="simple">
<li><strong>映像：</strong> OpenStack 的 Glance 管理着 VM 的映像。映像相对恒定， OpenStack 把它们当作大块二进制数据、并按需下载。</li>
<li><strong>卷宗：</strong> 卷宗是块设备， OpenStack 用它们引导虚拟机、或挂到运行着的虚拟机上。 OpenStack 用 Cinder 服务管理卷宗。</li>
<li><strong>Guest Disks</strong>: Guest disks are guest operating system disks. By default,
when you boot a virtual machine, its disk appears as a file on the filesystem
of the hypervisor (usually under <tt class="docutils literal"><span class="pre">/var/lib/nova/instances/&lt;uuid&gt;/</span></tt>). Prior
to OpenStack Havana, the only way to boot a VM in Ceph was to use the
boot-from-volume functionality of Cinder. However, now it is possible to boot
every virtual machine inside Ceph directly without using Cinder, which is
advantageous because it allows you to perform maintenance operations easily
with the live-migration process. Additionally, if your hypervisor dies it is
also convenient to trigger <tt class="docutils literal"><span class="pre">nova</span> <span class="pre">evacuate</span></tt> and  run the virtual machine
elsewhere almost seamlessly.</li>
</ul>
<p>你可以用 OpenStack Glance 把映像存储到 Ceph 块设备中，还可以用 Cinder 来引导映像的写时复制克隆品。</p>
<p>下面将详细指导你安装设置 Glance 、 Cinder 和 Nova ，虽然它们不一定一起用。你可以在本地硬盘上运行 VM 、却把映像存储于 Ceph 块设备，反之亦可。</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">Ceph doesn’t support QCOW2 for hosting a virtual machine disk.
Thus if you want to boot virtual machines in Ceph (ephemeral backend or boot
from volume), the Glance image format must be <tt class="docutils literal"><span class="pre">RAW</span></tt>.</p>
</div>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">This document describes using Ceph Block Devices with OpenStack Havana.
For earlier versions of OpenStack see
<a class="reference external" href="http://ceph.com/docs/dumpling/rbd/rbd-openstack">块设备与 OpenStack (Dumpling)</a>.</p>
</div>
<div class="section" id="create-a-pool">
<span id="index-1"></span><h2>Create a Pool<a class="headerlink" href="#create-a-pool" title="Permalink to this headline">¶</a></h2>
<p>默认情况下， Ceph 块设备使用 <tt class="docutils literal"><span class="pre">rbd</span></tt> 存储池，你可以用任何可用存储池。但我们建议分别为 Cinder 和 Glance 创建存储池。确保 Ceph 集群在运行，然后创建存储池。</p>
<div class="highlight-python"><pre>ceph osd pool create volumes 128
ceph osd pool create images 128
ceph osd pool create backups 128
ceph osd pool create vms 128</pre>
</div>
<p>参考<a class="reference external" href="../../rados/operations/pools#createpool">创建存储池</a>为存储池指定归置组数量，参考<a class="reference external" href="../../rados/operations/placement-groups">归置组</a>确定应该为存储池分配多少归置组。</p>
</div>
<div class="section" id="openstack-ceph">
<h2>配置 OpenStack 的 Ceph 客户端<a class="headerlink" href="#openstack-ceph" title="Permalink to this headline">¶</a></h2>
<p>运行着 <tt class="docutils literal"><span class="pre">glance-api</span></tt> 、 <tt class="docutils literal"><span class="pre">cinder-volume</span></tt> 、 <tt class="docutils literal"><span class="pre">nova-compute</span></tt> 或 <tt class="docutils literal"><span class="pre">cinder-backup</span></tt> 的主机被当作 Ceph 客户端，它们都需要 <tt class="docutils literal"><span class="pre">ceph.conf</span></tt> 文件。</p>
<div class="highlight-python"><pre>ssh {your-openstack-server} sudo tee /etc/ceph/ceph.conf &lt;/etc/ceph/ceph.conf</pre>
</div>
<div class="section" id="ceph">
<h3>安装 Ceph 客户端软件包<a class="headerlink" href="#ceph" title="Permalink to this headline">¶</a></h3>
<p>在运行 <tt class="docutils literal"><span class="pre">glance-api</span></tt> 的节点上你需要 <tt class="docutils literal"><span class="pre">librbd</span></tt> 的 Python 接口：</p>
<div class="highlight-python"><pre>sudo apt-get install python-rbd
sudo yum install python-rbd</pre>
</div>
<p>在 <tt class="docutils literal"><span class="pre">nova-compute</span></tt> 、 <tt class="docutils literal"><span class="pre">cinder-backup</span></tt> 和 <tt class="docutils literal"><span class="pre">cinder-volume</span></tt> 节点上，要安装 Python 绑定和客户端命令行工具：</p>
<div class="highlight-python"><pre>sudo apt-get install ceph-common
sudo yum install ceph</pre>
</div>
</div>
<div class="section" id="id3">
<h3>配置 Ceph 客户端认证<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>如果你启用了 <a class="reference external" href="../../rados/operations/authentication">cephx 认证</a>，需要分别为 Nova/Cinder 和 Glance 创建新用户。命令如下：</p>
<div class="highlight-python"><pre>ceph auth get-or-create client.cinder mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rx pool=images'
ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images'
ceph auth get-or-create client.cinder-backup mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=backups'</pre>
</div>
<p>把这些用户 <tt class="docutils literal"><span class="pre">client.cinder</span></tt> 、 <tt class="docutils literal"><span class="pre">client.glance</span></tt> 和 <tt class="docutils literal"><span class="pre">client.cinder-backup</span></tt> 的密钥环复制到各自所在节点，并修正所有权：</p>
<div class="highlight-python"><pre>ceph auth get-or-create client.glance | ssh {your-glance-api-server} sudo tee /etc/ceph/ceph.client.glance.keyring
ssh {your-glance-api-server} sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring
ceph auth get-or-create client.cinder | ssh {your-volume-server} sudo tee /etc/ceph/ceph.client.cinder.keyring
ssh {your-cinder-volume-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring
ceph auth get-or-create client.cinder-backup | ssh {your-cinder-backup-server} sudo tee /etc/ceph/ceph.client.cinder-backup.keyring
ssh {your-cinder-backup-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder-backup.keyring</pre>
</div>
<p>运行 <tt class="docutils literal"><span class="pre">nova-compute</span></tt> 的节点，其进程需要密钥环文件：</p>
<div class="highlight-python"><pre>ceph auth get-or-create client.cinder | ssh {your-nova-compute-server} sudo tee /etc/ceph/ceph.client.cinder.keyring</pre>
</div>
<p>还得把 <tt class="docutils literal"><span class="pre">client.cinder</span></tt> 用户的密钥存进 <tt class="docutils literal"><span class="pre">libvirt</span></tt> ， libvirt 进程从 Cinder 挂载块设备时要用它访问集群。</p>
<p>在运行 <tt class="docutils literal"><span class="pre">nova-compute</span></tt> 的节点上创建一个密钥的临时副本：</p>
<div class="highlight-python"><pre>ceph auth get-key client.cinder | ssh {your-compute-node} tee client.cinder.key</pre>
</div>
<p>然后，在计算节点上把密钥加进 <tt class="docutils literal"><span class="pre">libvirt</span></tt> 、然后删除临时副本：</p>
<div class="highlight-python"><pre>uuidgen
457eb676-33da-42ec-9a8c-9293d545c337

cat &gt; secret.xml &lt;&lt;EOF
&lt;secret ephemeral='no' private='no'&gt;
        &lt;uuid&gt;457eb676-33da-42ec-9a8c-9293d545c337&lt;/uuid&gt;
        &lt;usage type='ceph'&gt;
                &lt;name&gt;client.cinder secret&lt;/name&gt;
        &lt;/usage&gt;
&lt;/secret&gt;
EOF
sudo virsh secret-define --file secret.xml
Secret 457eb676-33da-42ec-9a8c-9293d545c337 created
sudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) &amp;&amp; rm client.cinder.key secret.xml</pre>
</div>
<p>保留密钥的 uuid ，稍后配置 <tt class="docutils literal"><span class="pre">nova-compute</span></tt> 要用。</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">You don&#8217;t necessarily need the UUID on all the compute nodes.
However from a platform consistency perspective, it&#8217;s better to keep the
same UUID.</p>
</div>
</div>
</div>
<div class="section" id="id4">
<h2>让 OpenStack 使用 Ceph<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<div class="section" id="glance">
<h3>配置 Glance<a class="headerlink" href="#glance" title="Permalink to this headline">¶</a></h3>
<p>Glance 可使用多种后端存储映像，要让它默认使用 Ceph 块设备，可以这样配置 Glance 。</p>
<div class="section" id="juno">
<h4>低于 Juno 的版本<a class="headerlink" href="#juno" title="Permalink to this headline">¶</a></h4>
<p>编辑 <tt class="docutils literal"><span class="pre">/etc/glance/glance-api.conf</span></tt> 并把下列内容加到 <tt class="docutils literal"><span class="pre">[DEFAULT]</span></tt> 段下：</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">default_store</span> <span class="o">=</span> <span class="n">rbd</span>
<span class="n">rbd_store_user</span> <span class="o">=</span> <span class="n">glance</span>
<span class="n">rbd_store_pool</span> <span class="o">=</span> <span class="n">images</span>
<span class="n">rbd_store_chunk_size</span> <span class="o">=</span> <span class="mi">8</span>
</pre></div>
</div>
</div>
<div class="section" id="id5">
<h4>Juno 版<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h4>
<p>编辑 <tt class="docutils literal"><span class="pre">/etc/glance/glance-api.conf</span></tt> 并把下列内容加到 <tt class="docutils literal"><span class="pre">[glance_store]</span></tt> 段下：</p>
<div class="highlight-python"><pre>[DEFAULT]
...
default_store = rbd
...
[glance_store]
stores = rbd
rbd_store_pool = images
rbd_store_user = glance
rbd_store_ceph_conf = /etc/ceph/ceph.conf
rbd_store_chunk_size = 8</pre>
</div>
<p>关于 Glance 里可用的其它配置选项见 <a class="reference external" href="http://docs.openstack.org/trunk/config-reference/content/section_glance-api.conf.html">http://docs.openstack.org/trunk/config-reference/content/section_glance-api.conf.html</a>.</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">Glance 还没完全迁移到 &#8216;store&#8217; ，所以我们还得在 DEFAULT 段下配置 store 。</p>
</div>
</div>
<div class="section" id="id6">
<h4>任意版 OpenStack<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<p>如果你想允许使用映像的写时复制克隆品，还得把下列内容加到 <tt class="docutils literal"><span class="pre">[DEFAULT]</span></tt> 段下：</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">show_image_direct_url</span> <span class="o">=</span> <span class="bp">True</span>
</pre></div>
</div>
<p>注意，这里通过 Glance 的 API 展示了后端位置，所以此选项启用时的入口不能公开访问。</p>
<p>禁用 Glance 缓存管理，以免映像被缓存到 <tt class="docutils literal"><span class="pre">/var/lib/glance/image-cache/</span></tt> 下；假设你的配置文件里有 <tt class="docutils literal"><span class="pre">flavor</span> <span class="pre">=</span> <span class="pre">keystone+cachemanagement</span></tt></p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">paste_deploy</span><span class="p">]</span>
<span class="n">flavor</span> <span class="o">=</span> <span class="n">keystone</span>
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h4>映像属性<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h4>
<p>我们建议你配置如下映像属性：</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">hw_scsi_model=virtio-scsi</span></tt>: 添加 virtio-scsi 控制器以获得更好的性能、并支持 discard 操作；</li>
<li><tt class="docutils literal"><span class="pre">hw_disk_bus=scsi</span></tt>: 把所有 cinder 块设备都连到这个控制器；</li>
<li><tt class="docutils literal"><span class="pre">hw_qemu_guest_agent=yes</span></tt>: 启用 QEMU guest agent （访客代理）</li>
<li><tt class="docutils literal"><span class="pre">os_require_quiesce=yes</span></tt>: 通过 QEMU guest agent 向外发送文件系统的 freeze/thaw 调用</li>
</ul>
</div>
</div>
<div class="section" id="cinder">
<h3>配置 Cinder<a class="headerlink" href="#cinder" title="Permalink to this headline">¶</a></h3>
<p>OpenStack 需要一个驱动和 Ceph 块设备交互，还得指定块设备所在的存储池名字。编辑 OpenStack 节点上的 <tt class="docutils literal"><span class="pre">/etc/cinder/cinder.conf</span></tt> ，添加：</p>
<div class="highlight-python"><pre>volume_driver = cinder.volume.drivers.rbd.RBDDriver
rbd_pool = volumes
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_flatten_volume_from_snapshot = false
rbd_max_clone_depth = 5
rbd_store_chunk_size = 4
rados_connect_timeout = -1
glance_api_version = 2</pre>
</div>
<p>如果你在用 <a class="reference external" href="../../rados/operations/authentication">cephx 认证</a>，还需要配置用户及其密钥（前述文档中存进了 <tt class="docutils literal"><span class="pre">libvirt</span></tt> ）的 uuid ：</p>
<blockquote>
<div>rbd_user = cinder
rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337</div></blockquote>
<p>Note that if you are configuring multiple cinder back ends,
<tt class="docutils literal"><span class="pre">glance_api_version</span> <span class="pre">=</span> <span class="pre">2</span></tt> must be in the <tt class="docutils literal"><span class="pre">[DEFAULT]</span></tt> section.</p>
</div>
<div class="section" id="configuring-cinder-backup">
<h3>Configuring Cinder Backup<a class="headerlink" href="#configuring-cinder-backup" title="Permalink to this headline">¶</a></h3>
<p>OpenStack Cinder Backup requires a specific daemon so don&#8217;t forget to install it.
On your Cinder Backup node, edit <tt class="docutils literal"><span class="pre">/etc/cinder/cinder.conf</span></tt> and add:</p>
<div class="highlight-python"><pre>backup_driver = cinder.backup.drivers.ceph
backup_ceph_conf = /etc/ceph/ceph.conf
backup_ceph_user = cinder-backup
backup_ceph_chunk_size = 134217728
backup_ceph_pool = backups
backup_ceph_stripe_unit = 0
backup_ceph_stripe_count = 0
restore_discard_excess_bytes = true</pre>
</div>
</div>
<div class="section" id="configuring-nova-to-attach-ceph-rbd-block-device">
<h3>Configuring Nova to attach Ceph RBD block device<a class="headerlink" href="#configuring-nova-to-attach-ceph-rbd-block-device" title="Permalink to this headline">¶</a></h3>
<p>In order to attach Cinder devices (either normal block or by issuing a boot
from volume), you must tell Nova (and libvirt) which user and UUID to refer to
when attaching the device. libvirt will refer to this user when connecting and
authenticating with the Ceph cluster.</p>
<div class="highlight-python"><pre>rbd_user = cinder
rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337</pre>
</div>
<p>These two flags are also used by the Nova ephemeral backend.</p>
</div>
<div class="section" id="nova">
<h3>Nova 的配置<a class="headerlink" href="#nova" title="Permalink to this headline">¶</a></h3>
<p>要让所有虚拟机直接从 Ceph 引导，必须配置 Nova 的 ephemeral 后端。</p>
<p>我们建议在 Ceph 配置文件里启用 RBD 缓存（从 Giant 起默认启用）；另外，启用管理套接字对于故障排查来说大有好处，给每个使用 Ceph 块设备的虚拟机分配一个套接字有助于调查性能和/或异常行为。</p>
<p>可以这样访问套接字：</p>
<div class="highlight-python"><pre>ceph daemon /var/run/ceph/ceph-client.cinder.19195.32310016.asok help</pre>
</div>
<p>编辑所有计算节点上的 Ceph 配置文件：</p>
<div class="highlight-python"><pre>[client]
        rbd cache = true
        rbd cache writethrough until flush = true
        admin socket = /var/run/ceph/guests/$cluster-$type.$id.$pid.$cctid.asok
        log file = /var/log/qemu/qemu-guest-$pid.log
        rbd concurrent management ops = 20</pre>
</div>
<p>调整这些路径的权限：</p>
<div class="highlight-python"><pre>mkdir -p /var/run/ceph/guests/ /var/log/qemu/
chown qemu:libvirtd /var/run/ceph/guests /var/log/qemu/</pre>
</div>
<p>要注意， <tt class="docutils literal"><span class="pre">qemu</span></tt> 用户和 <tt class="docutils literal"><span class="pre">libvirtd</span></tt> 组可能因系统不同而不同，前面的实例基于 RedHat 风格的系统。</p>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">如果你的虚拟机已经跑起来了，重启一下就能得到套接字。</p>
</div>
<div class="section" id="havana-and-icehouse">
<h4>Havana and Icehouse<a class="headerlink" href="#havana-and-icehouse" title="Permalink to this headline">¶</a></h4>
<p>Havana and Icehouse require patches to implement copy-on-write cloning and fix
bugs with image size and live migration of ephemeral disks on rbd. These are
available in branches based on upstream Nova <a class="reference external" href="https://github.com/jdurgin/nova/tree/havana-ephemeral-rbd">stable/havana</a>  and
<a class="reference external" href="https://github.com/angdraug/nova/tree/rbd-ephemeral-clone-stable-icehouse">stable/icehouse</a>. Using them is not mandatory but <strong>highly recommended</strong> in
order to take advantage of the copy-on-write clone functionality.</p>
<p>On every Compute node, edit <tt class="docutils literal"><span class="pre">/etc/nova/nova.conf</span></tt> and add:</p>
<div class="highlight-python"><pre>libvirt_images_type = rbd
libvirt_images_rbd_pool = vms
libvirt_images_rbd_ceph_conf = /etc/ceph/ceph.conf
libvirt_disk_cachemodes="network=writeback"
rbd_user = cinder
rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337</pre>
</div>
<p>It is also a good practice to disable file injection. While booting an
instance, Nova usually attempts to open the rootfs of the virtual machine.
Then, Nova injects values such as password, ssh keys etc. directly into the
filesystem. However, it is better to rely on the metadata service and
<tt class="docutils literal"><span class="pre">cloud-init</span></tt>.</p>
<p>On every Compute node, edit <tt class="docutils literal"><span class="pre">/etc/nova/nova.conf</span></tt> and add:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">libvirt_inject_password</span> <span class="o">=</span> <span class="n">false</span>
<span class="n">libvirt_inject_key</span> <span class="o">=</span> <span class="n">false</span>
<span class="n">libvirt_inject_partition</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
</pre></div>
</div>
<p>为确保在线迁移能顺利进行，要使用如下标志：</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">libvirt_live_migration_flag</span><span class="o">=</span><span class="s">&quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="id8">
<h4>Juno<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h4>
<p>In Juno, Ceph block device was moved under the <tt class="docutils literal"><span class="pre">[libvirt]</span></tt> section.
On every Compute node, edit <tt class="docutils literal"><span class="pre">/etc/nova/nova.conf</span></tt> under the <tt class="docutils literal"><span class="pre">[libvirt]</span></tt>
section and add:</p>
<div class="highlight-python"><pre>[libvirt]
images_type = rbd
images_rbd_pool = vms
images_rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_user = cinder
rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337
disk_cachemodes="network=writeback"</pre>
</div>
<p>It is also a good practice to disable file injection. While booting an
instance, Nova usually attempts to open the rootfs of the virtual machine.
Then, Nova injects values such as password, ssh keys etc. directly into the
filesystem. However, it is better to rely on the metadata service and
<tt class="docutils literal"><span class="pre">cloud-init</span></tt>.</p>
<p>On every Compute node, edit <tt class="docutils literal"><span class="pre">/etc/nova/nova.conf</span></tt> and add the following
under the <tt class="docutils literal"><span class="pre">[libvirt]</span></tt> section:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">inject_password</span> <span class="o">=</span> <span class="n">false</span>
<span class="n">inject_key</span> <span class="o">=</span> <span class="n">false</span>
<span class="n">inject_partition</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
</pre></div>
</div>
<p>为确保在线迁移能顺利进行，要使用如下标志（写到 <tt class="docutils literal"><span class="pre">[libvirt]</span></tt> 段下）：</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">live_migration_flag</span><span class="o">=</span><span class="s">&quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="kilo">
<h4>Kilo<a class="headerlink" href="#kilo" title="Permalink to this headline">¶</a></h4>
<p>为虚拟机的 ephemeral 根磁盘启用 discard 功能：</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">libvirt</span><span class="p">]</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="n">hw_disk_discard</span> <span class="o">=</span> <span class="n">unmap</span> <span class="c"># 启用 discard 功能（注意性能）</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id9">
<h2>重启 OpenStack<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<p>要激活 Ceph 块设备驱动、并把块设备存储池名载入配置，必须重启 OpenStack 。在基于 Debian 的系统上需在对应节点上执行这些命令：</p>
<div class="highlight-python"><pre>sudo glance-control api restart
sudo service nova-compute restart
sudo service cinder-volume restart
sudo service cinder-backup restart</pre>
</div>
<p>在基于 Red Hat 的系统上执行：</p>
<div class="highlight-python"><pre>sudo service openstack-glance-api restart
sudo service openstack-nova-compute restart
sudo service openstack-cinder-volume restart
sudo service openstack-cinder-backup restart</pre>
</div>
<p>一旦 OpenStack 启动并运行正常，应该就可以创建卷宗并用它引导了。</p>
</div>
<div class="section" id="id10">
<h2>从块设备引导<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<p>你可以用 Cinder 命令行工具从一映像创建卷宗：</p>
<div class="highlight-python"><pre>cinder create --image-id {id of image} --display-name {name of volume} {size of volume}</pre>
</div>
<p>注意映像必须是 RAW 格式，你可以用 <a class="reference external" href="../qemu-rbd/#running-qemu-with-rbd">qemu-img</a> 转换格式，如：</p>
<div class="highlight-python"><pre>qemu-img convert -f {source-format} -O {output-format} {source-filename} {output-filename}
qemu-img convert -f qcow2 -O raw precise-cloudimg.img precise-cloudimg.raw</pre>
</div>
<p>Glance 和 Cinder 都使用 Ceph 块设备时，此镜像又是个写时复制克隆，就能非常快地创建新卷宗。在 OpenStack 操作板里就能从那个卷宗引导，步骤如下：</p>
<ol class="arabic simple">
<li>启动新例程；</li>
<li>选择与写时复制克隆关联的镜像；</li>
<li>选中 &#8216;boot from volume&#8217; ；</li>
<li>选中你刚创建的卷宗。</li>
</ol>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../">
              <img class="logo" src="../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../start/intro/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../start/">安装（快速）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/">安装（手动）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../rbd/">Ceph 块设备</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../rados-rbd-cmds/">命令</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-ko/">内核模块</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-snapshot/">快照</a></li>
<li class="toctree-l2"><a class="reference internal" href="../qemu-rbd/">QEMU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../libvirt/">libvirt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-config-ref/">缓存选项</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="">OpenStack</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#create-a-pool">Create a Pool</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openstack-ceph">配置 OpenStack 的 Ceph 客户端</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ceph">安装 Ceph 客户端软件包</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">配置 Ceph 客户端认证</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id4">让 OpenStack 使用 Ceph</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#glance">配置 Glance</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#juno">低于 Juno 的版本</a></li>
<li class="toctree-l5"><a class="reference internal" href="#id5">Juno 版</a></li>
<li class="toctree-l5"><a class="reference internal" href="#id6">任意版 OpenStack</a></li>
<li class="toctree-l5"><a class="reference internal" href="#id7">映像属性</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#cinder">配置 Cinder</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-cinder-backup">Configuring Cinder Backup</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-nova-to-attach-ceph-rbd-block-device">Configuring Nova to attach Ceph RBD block device</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nova">Nova 的配置</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#havana-and-icehouse">Havana and Icehouse</a></li>
<li class="toctree-l5"><a class="reference internal" href="#id8">Juno</a></li>
<li class="toctree-l5"><a class="reference internal" href="#kilo">Kilo</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id9">重启 OpenStack</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id10">从块设备引导</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-cloudstack/">CloudStack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../man/8/rbd/">rbd 手册页</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../man/8/rbd-fuse/">rbd-fuse 手册页</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../man/8/rbd-nbd/">rbd-nbd 手册页</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../man/8/ceph-rbdnamer/">ceph-rbdnamer 手册页</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-replay/">RBD 重放</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../man/8/rbd-replay-prep/">rbd-replay-prep 手册页</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../man/8/rbd-replay/">rbd-replay 手册页</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../man/8/rbd-replay-many/">rbd-replay-many 手册页</a></li>
<li class="toctree-l2"><a class="reference internal" href="../librbdpy/">librbd 的 Python 接口</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/">开发文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes/">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/">Ceph 版本</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary/">Ceph 术语</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../rbd-cloudstack/" title="块设备与 CloudStack"
             >next</a> |</li>
        <li class="right" >
          <a href="../rbd-config-ref/" title="librbd 选项"
             >previous</a> |</li>
        <li><a href="../../">Ceph Documentation</a> &raquo;</li>
          <li><a href="../rbd/" >Ceph 块设备</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2010-2014, Inktank Storage, Inc. and contributors. Licensed under Creative Commons BY-SA.
    </div>
  </body>
</html>
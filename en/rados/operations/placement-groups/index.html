
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Placement Groups &mdash; Ceph Documentation</title>
    
    <link rel="stylesheet" href="../../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../',
        VERSION:     'dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    <link rel="top" title="Ceph Documentation" href="../../../../" />
    <script type="text/javascript" src="http://ayni.ceph.com/public/js/ceph.js"></script>

  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../../../../">Ceph Documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="placement-groups">
<h1>Placement Groups<a class="headerlink" href="#placement-groups" title="Permalink to this headline">¶</a></h1>
<div class="section" id="a-preselection-of-pg-num">
<span id="preselection"></span><h2>A preselection of pg_num<a class="headerlink" href="#a-preselection-of-pg-num" title="Permalink to this headline">¶</a></h2>
<p>When creating a new pool with:</p>
<div class="highlight-python"><pre>ceph osd pool create {pool-name} pg_num</pre>
</div>
<p>it is mandatory to choose the value of <tt class="docutils literal"><span class="pre">pg_num</span></tt> because it cannot be
calculated automatically. Here are a few values commonly used:</p>
<ul class="simple">
<li>Less than 5 OSDs set <tt class="docutils literal"><span class="pre">pg_num</span></tt> to 128</li>
<li>Between 5 and 10 OSDs set <tt class="docutils literal"><span class="pre">pg_num</span></tt> to 512</li>
<li>Between 10 and 50 OSDs set <tt class="docutils literal"><span class="pre">pg_num</span></tt> to 4096</li>
<li>If you have more than 50 OSDs, you need to understand the tradeoffs
and how to calculate the <tt class="docutils literal"><span class="pre">pg_num</span></tt> value by yourself</li>
</ul>
<p>As the number of OSDs increases, chosing the right value for pg_num
becomes more important because it has a significant influence on the
behavior of the cluster as well as the durability of the data when
something goes wrong (i.e. the probability that a catastrophic event
leads to data loss).</p>
</div>
<div class="section" id="how-are-placement-groups-used">
<h2>How are Placement Groups used ?<a class="headerlink" href="#how-are-placement-groups-used" title="Permalink to this headline">¶</a></h2>
<p>A placement group (PG) aggregates objects within a pool because
tracking object placement and object metadata on a per-object basis is
computationally expensive&#8211;i.e., a system with millions of objects
cannot realistically track placement on a per-object basis.</p>
<p class="ditaa">
<img src="../../../../_images/ditaa-1fde157d24b63e3b465d96eb6afea22078c85a90.png"/>
</p>
<p>Placement groups are invisible to the Ceph user: the CRUSH algorithm
determines in which placement group the object will be
placed. Although CRUSH is a deterministic function using the object
name as a parameter, there is no way to force an object into a given
placement group.</p>
<p>The object&#8217;s contents within a placement group are stored in a set of
OSDs. For instance, in a replicated pool of size two, each placement
group will store objects on two OSDs, as shown below.</p>
<p class="ditaa">
<img src="../../../../_images/ditaa-3c86866fb6edc99dad6ccf51e25e536806f0b079.png"/>
</p>
<p>Should OSD #2 fail, another will be assigned to Placement Group #1 and
will be filled with copies of all objects in OSD #1. If the pool size
is changed from two to three, an additional OSD will be assigned to
the placement group and will receive copies of all objects in the
placement group.</p>
<p>Placement groups do not own the OSD, they share it with other
placement groups from the same pool or even other pools. If OSD #2
fails, the Placement Group #2 will also have to restore copies of
objects, using OSD #3.</p>
<p>When the number of placement groups increases, the new placement
groups will be assigned OSDs. The result of the CRUSH function will
also change and some objects from the former placement groups will be
copied over to the new Placement Groups and removed from the old ones.</p>
</div>
<div class="section" id="placement-groups-tradeoffs">
<h2>Placement Groups Tradeoffs<a class="headerlink" href="#placement-groups-tradeoffs" title="Permalink to this headline">¶</a></h2>
<p>Data durability and even distribution among all OSDs call for more
placement groups but their number should be reduced to the minimum to
save CPU and memory.</p>
<div class="section" id="data-durability">
<span id="id1"></span><h3>Data durability<a class="headerlink" href="#data-durability" title="Permalink to this headline">¶</a></h3>
<p>After an OSD fails, the risk of data loss increases until the data it
contained is fully recovered. Let&#8217;s imagine a scenario that causes
permanent data loss in a single placement group:</p>
<ul class="simple">
<li>The OSD fails and all copies of the object it contains are lost.
For all objects within the placement group the number of replica
suddently drops from three to two.</li>
<li>Ceph starts recovery for this placement group by chosing a new OSD
to re-create the third copy of all objects.</li>
<li>Another OSD, within the same placement group, fails before the new
OSD is fully populated with the third copy. Some objects will then
only have one surviving copies.</li>
<li>Ceph picks yet another OSD and keeps copying objects to restore the
desired number of copies.</li>
<li>A third OSD, within the same placement group, fails before recovery
is complete. If this OSD contained the only remaining copy of an
object, it is permanently lost.</li>
</ul>
<p>In a cluster containing 10 OSDs with 512 placement groups in a three
replica pool, CRUSH will give each placement groups three OSDs. In the
end, each OSDs will end up hosting (512 * 3) / 10 = ~150 Placement
Groups. When the first OSD fails, the above scenario will therefore
start recovery for all 150 placement groups at the same time.</p>
<p>The 150 placement groups being recovered are likely to be
homogeneously spread over the 9 remaining OSDs. Each remaining OSD is
therefore likely to send copies of objects to all others and also
receive some new objects to be stored because they became part of a
new placement group.</p>
<p>The amount of time it takes for this recovery to complete entirely
depends on the architecture of the Ceph cluster. Let say each OSD is
hosted by a 1TB SSD on a single machine and all of them are connected
to a 10Gb/s switch and the recovery for a single OSD completes within
M minutes. If there are two OSDs per machine using spinners with no
SSD journal and a 1Gb/s switch, it will at least be an order of
magnitude slower.</p>
<p>In a cluster of this size, the number of placement groups has almost
no influence on data durability. It could be 128 or 8192 and the
recovery would not be slower or faster.</p>
<p>However, growing the same Ceph cluster to 20 OSDs instead of 10 OSDs
is likely to speed up recovery and therefore improve data durability
significantly. Each OSD now participates in only ~75 placement groups
instead of ~150 when there were only 10 OSDs and it will still require
all 19 remaining OSDs to perform the same amount of object copies in
order to recover. But where 10 OSDs had to copy approximately 100GB
each, they now have to copy 50GB each instead. If the network was the
bottleneck, recovery will happen twice as fast. In other words,
recovery goes faster when the number of OSDs increases.</p>
<p>If this cluster grows to 40 OSDs, each of them will only host ~35
placement groups. If an OSD dies, recovery will keep going faster
unless it is blocked by another bottleneck. However, if this cluster
grows to 200 OSDs, each of them will only host ~7 placement groups. If
an OSD dies, recovery will happen between at most of ~21 (7 * 3) OSDs
in these placement groups: recovery will take longer than when there
were 40 OSDs, meaning the number of placement groups should be
increased.</p>
<p>No matter how short the recovery time is, there is a chance for a
second OSD to fail while it is in progress. In the 10 OSDs cluster
described above, if any of them fail, then ~17 placement groups
(i.e. ~150 / 9 placement groups being recovered) will only have one
surviving copy. And if any of the 8 remaining OSD fail, the last
objects of two placement groups are likely to be lost (i.e. ~17 / 8
placement groups with only one remaining copy being recovered).</p>
<p>When the size of the cluster grows to 20 OSDs, the number of Placement
Groups damaged by the loss of three OSDs drops. The second OSD lost
will degrade ~4 (i.e. ~75 / 19 placement groups being recovered)
instead of ~17 and the third OSD lost will only lose data if it is one
of the four OSDs containing the surviving copy. In other words, if the
probability of losing one OSD is 0.0001% during the recovery time
frame, it goes from 17 * 10 * 0.0001% in the cluster with 10 OSDs to 4 * 20 *
0.0001% in the cluster with 20 OSDs.</p>
<p>In a nutshell, more OSDs mean faster recovery and a lower risk of
cascading failures leading to the permanent loss of a Placement
Group. Having 512 or 4096 Placement Groups is roughly equivalent in a
cluster with less than 50 OSDs as far as data durability is concerned.</p>
<p>Note: It may take a long time for a new OSD added to the cluster to be
populated with placement groups that were assigned to it. However
there is no degradation of any object and it has no impact on the
durability of the data contained in the Cluster.</p>
</div>
<div class="section" id="object-distribution-within-a-pool">
<span id="object-distribution"></span><h3>Object distribution within a pool<a class="headerlink" href="#object-distribution-within-a-pool" title="Permalink to this headline">¶</a></h3>
<p>Ideally objects are evenly distributed in each placement group. Since
CRUSH computes the placement group for each object, but does not
actually know how much data is stored in each OSD within this
placement group, the ratio between the number of placement groups and
the number of OSDs may influence the distribution of the data
significantly.</p>
<p>For instance, if there was single a placement group for ten OSDs in a
three replica pool, only three OSD would be used because CRUSH would
have no other choice. When more placement groups are available,
objects are more likely to be evenly spread among them. CRUSH also
makes every effort to evenly spread OSDs among all existing Placement
Groups.</p>
<p>As long as there are one or two orders of magnitude more Placement
Groups than OSDs, the distribution should be even. For instance, 300
placement groups for 3 OSDs, 1000 placement groups for 10 OSDs etc.</p>
<p>Uneven data distribution can be caused by factors other than the ratio
between OSDs and placement groups. Since CRUSH does not take into
account the size of the objects, a few very large objects may create
an imbalance. Let say one million 4K objects totaling 4GB are evenly
spread among 1000 placement groups on 10 OSDs. They will use 4GB / 10
= 400MB on each OSD. If one 400MB object is added to the pool, the
three OSDs supporting the placement group in which the object has been
placed will be filled with 400MB + 400MB = 800MB while the seven
others will remain occupied with only 400MB.</p>
</div>
<div class="section" id="memory-cpu-and-network-usage">
<span id="resource-usage"></span><h3>Memory, CPU and network usage<a class="headerlink" href="#memory-cpu-and-network-usage" title="Permalink to this headline">¶</a></h3>
<p>For each placement group, OSDs and MONs need memory, network and CPU
at all times and even more during recovery. Sharing this overhead by
clustering objects within a placement group is one of the main reasons
they exist.</p>
<p>Minimizing the number of placement groups saves significant amounts of
resources.</p>
</div>
</div>
<div class="section" id="choosing-the-number-of-placement-groups">
<h2>Choosing the number of Placement Groups<a class="headerlink" href="#choosing-the-number-of-placement-groups" title="Permalink to this headline">¶</a></h2>
<p>If you have more than 50 OSDs, we recommend approximately 50-100
placement groups per OSD to balance out resource usage, data
durability and distribution. If you have less than 50 OSDs, chosing
among the <a class="reference internal" href="#preselection">preselection</a> above is best. For a single pool of objects,
you can use the following formula to get a baseline:</p>
<div class="highlight-python"><pre>             (OSDs * 100)
Total PGs =  ------------
              pool size</pre>
</div>
<p>Where <strong>pool size</strong> is either the number of replicas for replicated
pools or the K+M sum for erasure coded pools (as returned by <strong>ceph
osd erasure-code-profile get</strong>).</p>
<p>You should then check if the result makes sense with the way you
designed your Ceph cluster to maximize <a class="reference internal" href="#data-durability">data durability</a>,
<a class="reference internal" href="#object-distribution">object distribution</a> and minimize <a class="reference internal" href="#resource-usage">resource usage</a>.</p>
<p>The result should be <strong>rounded up to the nearest power of two.</strong>
Rounding up is optional, but recommended for CRUSH to evenly balance
the number of objects among placement groups.</p>
<p>As an example, for a cluster with 200 OSDs and a pool size of 3
replicas, you would estimate your number of PGs as follows:</p>
<div class="highlight-python"><pre>(200 * 100)
----------- = 6667. Nearest power of 2: 8192
     3</pre>
</div>
<p>When using multiple data pools for storing objects, you need to ensure
that you balance the number of placement groups per pool with the
number of placement groups per OSD so that you arrive at a reasonable
total number of placement groups that provides reasonably low variance
per OSD without taxing system resources or making the peering process
too slow.</p>
<p>For instance a cluster of 10 pools each with 512 placement groups on
ten OSDs is a total of 5,120 placement groups spread over ten OSDs,
that is 512 placement groups per OSD. That does not use too many
resources. However, if 1,000 pools were created with 512 placement
groups each, the OSDs will handle ~50,000 placement groups each and it
would require significantly more resources and time for peering.</p>
</div>
<div class="section" id="set-the-number-of-placement-groups">
<span id="setting-the-number-of-placement-groups"></span><h2>Set the Number of Placement Groups<a class="headerlink" href="#set-the-number-of-placement-groups" title="Permalink to this headline">¶</a></h2>
<p>To set the number of placement groups in a pool, you must specify the
number of placement groups at the time you create the pool.
See <a class="reference external" href="../pools#createpool">Create a Pool</a> for details. Once you&#8217;ve set placement groups for a
pool, you may increase the number of placement groups (but you cannot
decrease the number of placement groups). To increase the number of
placement groups, execute the following:</p>
<div class="highlight-python"><pre>ceph osd pool set {pool-name} pg_num {pg_num}</pre>
</div>
<p>Once you increase the number of placement groups, you must also
increase the number of placement groups for placement (<tt class="docutils literal"><span class="pre">pgp_num</span></tt>) before your
cluster will rebalance. The <tt class="docutils literal"><span class="pre">pgp_num</span></tt> should be equal to the <tt class="docutils literal"><span class="pre">pg_num</span></tt>.
To increase the number of placement groups for placement, execute the
following:</p>
<div class="highlight-python"><pre>ceph osd pool set {pool-name} pgp_num {pgp_num}</pre>
</div>
</div>
<div class="section" id="get-the-number-of-placement-groups">
<h2>Get the Number of Placement Groups<a class="headerlink" href="#get-the-number-of-placement-groups" title="Permalink to this headline">¶</a></h2>
<p>To get the number of placement groups in a pool, execute the following:</p>
<div class="highlight-python"><pre>ceph osd pool get {pool-name} pg_num</pre>
</div>
</div>
<div class="section" id="get-a-cluster-s-pg-statistics">
<h2>Get a Cluster&#8217;s PG Statistics<a class="headerlink" href="#get-a-cluster-s-pg-statistics" title="Permalink to this headline">¶</a></h2>
<p>To get the statistics for the placement groups in your cluster, execute the following:</p>
<div class="highlight-python"><pre>ceph pg dump [--format {format}]</pre>
</div>
<p>Valid formats are <tt class="docutils literal"><span class="pre">plain</span></tt> (default) and <tt class="docutils literal"><span class="pre">json</span></tt>.</p>
</div>
<div class="section" id="get-statistics-for-stuck-pgs">
<h2>Get Statistics for Stuck PGs<a class="headerlink" href="#get-statistics-for-stuck-pgs" title="Permalink to this headline">¶</a></h2>
<p>To get the statistics for all placement groups stuck in a specified state,
execute the following:</p>
<div class="highlight-python"><pre>ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format &lt;format&gt;] [-t|--threshold &lt;seconds&gt;]</pre>
</div>
<p><strong>Inactive</strong> Placement groups cannot process reads or writes because they are waiting for an OSD
with the most up-to-date data to come up and in.</p>
<p><strong>Unclean</strong> Placement groups contain objects that are not replicated the desired number
of times. They should be recovering.</p>
<p><strong>Stale</strong> Placement groups are in an unknown state - the OSDs that host them have not
reported to the monitor cluster in a while (configured by <tt class="docutils literal"><span class="pre">mon_osd_report_timeout</span></tt>).</p>
<p>Valid formats are <tt class="docutils literal"><span class="pre">plain</span></tt> (default) and <tt class="docutils literal"><span class="pre">json</span></tt>. The threshold defines the minimum number
of seconds the placement group is stuck before including it in the returned statistics
(default 300 seconds).</p>
</div>
<div class="section" id="get-a-pg-map">
<h2>Get a PG Map<a class="headerlink" href="#get-a-pg-map" title="Permalink to this headline">¶</a></h2>
<p>To get the placement group map for a particular placement group, execute the following:</p>
<div class="highlight-python"><pre>ceph pg map {pg-id}</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph pg map 1.6c</pre>
</div>
<p>Ceph will return the placement group map, the placement group, and the OSD status:</p>
<div class="highlight-python"><pre>osdmap e13 pg 1.6c (1.6c) -&gt; up [1,0] acting [1,0]</pre>
</div>
</div>
<div class="section" id="get-a-pgs-statistics">
<h2>Get a PGs Statistics<a class="headerlink" href="#get-a-pgs-statistics" title="Permalink to this headline">¶</a></h2>
<p>To retrieve statistics for a particular placement group, execute the following:</p>
<div class="highlight-python"><pre>ceph pg {pg-id} query</pre>
</div>
</div>
<div class="section" id="scrub-a-placement-group">
<h2>Scrub a Placement Group<a class="headerlink" href="#scrub-a-placement-group" title="Permalink to this headline">¶</a></h2>
<p>To scrub a placement group, execute the following:</p>
<div class="highlight-python"><pre>ceph pg scrub {pg-id}</pre>
</div>
<p>Ceph checks the primary and any replica nodes, generates a catalog of all objects
in the placement group and compares them to ensure that no objects are missing
or mismatched, and their contents are consistent.  Assuming the replicas all
match, a final semantic sweep ensures that all of the snapshot-related object
metadata is consistent. Errors are reported via logs.</p>
</div>
<div class="section" id="revert-lost">
<h2>Revert Lost<a class="headerlink" href="#revert-lost" title="Permalink to this headline">¶</a></h2>
<p>If the cluster has lost one or more objects, and you have decided to
abandon the search for the lost data, you must mark the unfound objects
as <tt class="docutils literal"><span class="pre">lost</span></tt>.</p>
<p>If all possible locations have been queried and objects are still
lost, you may have to give up on the lost objects. This is
possible given unusual combinations of failures that allow the cluster
to learn about writes that were performed before the writes themselves
are recovered.</p>
<p>Currently the only supported option is &#8220;revert&#8221;, which will either roll back to
a previous version of the object or (if it was a new object) forget about it
entirely. To mark the &#8220;unfound&#8221; objects as &#8220;lost&#8221;, execute the following:</p>
<div class="highlight-python"><pre>ceph pg {pg-id} mark_unfound_lost revert|delete</pre>
</div>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">Use this feature with caution, because it may confuse
applications that expect the object(s) to exist.</p>
</div>
<div class="toctree-wrapper compound">
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../../">
              <img class="logo" src="../../../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../../../">Table Of Contents</a></h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../start/intro/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../start/">安装（快速）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../install/">安装（手动）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rbd/rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dev/">开发文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../release-notes/">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../releases/">发布时间表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../glossary/">Ceph 术语</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../../../../">Ceph Documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2010-2014, Inktank Storage, Inc. and contributors. Licensed under Creative Commons BY-SA.
    </div>
  </body>
</html>
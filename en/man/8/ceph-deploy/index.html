
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>ceph-deploy – Ceph deployment tool &mdash; Ceph Documentation</title>
    
    <link rel="stylesheet" href="../../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../',
        VERSION:     'dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    <link rel="top" title="Ceph Documentation" href="../../../../" />
    <script type="text/javascript" src="http://ayni.ceph.com/public/js/ceph.js"></script>

  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../../../../">Ceph Documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="ceph-deploy-ceph-deployment-tool">
<h1>ceph-deploy &#8211; Ceph deployment tool<a class="headerlink" href="#ceph-deploy-ceph-deployment-tool" title="Permalink to this headline">¶</a></h1>
<div class="section" id="synopsis">
<h2>Synopsis<a class="headerlink" href="#synopsis" title="Permalink to this headline">¶</a></h2>
<div class="line-block">
<div class="line"><strong>ceph-deploy</strong> <strong>new</strong> [<em>initial-monitor-node(s)</em>]</div>
</div>
<div class="line-block">
<div class="line"><strong>ceph-deploy</strong> <strong>install</strong> [<em>ceph-node</em>] [<em>ceph-node</em>...]</div>
</div>
<div class="line-block">
<div class="line"><strong>ceph-deploy</strong> <strong>mon</strong> <em>create-initial</em></div>
</div>
<div class="line-block">
<div class="line"><strong>ceph-deploy</strong> <strong>osd</strong> <em>prepare</em> [<em>ceph-node</em>]:[<em>dir-path</em>]</div>
</div>
<div class="line-block">
<div class="line"><strong>ceph-deploy</strong> <strong>osd</strong> <em>activate</em> [<em>ceph-node</em>]:[<em>dir-path</em>]</div>
</div>
<div class="line-block">
<div class="line"><strong>ceph-deploy</strong> <strong>osd</strong> <em>create</em> [<em>ceph-node</em>]:[<em>dir-path</em>]</div>
</div>
<div class="line-block">
<div class="line"><strong>ceph-deploy</strong> <strong>admin</strong> [<em>admin-node</em>][<em>ceph-node</em>...]</div>
</div>
<div class="line-block">
<div class="line"><strong>ceph-deploy</strong> <strong>purgedata</strong> [<em>ceph-node</em>][<em>ceph-node</em>...]</div>
</div>
<div class="line-block">
<div class="line"><strong>ceph-deploy</strong> <strong>forgetkeys</strong></div>
</div>
</div>
<div class="section" id="description">
<h2>Description<a class="headerlink" href="#description" title="Permalink to this headline">¶</a></h2>
<p><strong class="program">ceph-deploy</strong> is a tool which allows easy and quick deployment of a
Ceph cluster without involving complex and detailed manual configuration. It
uses ssh to gain access to other Ceph nodes from the admin node, sudo for
administrator privileges on them and the underlying Python scripts automates
the manual process of Ceph installation on each node from the admin node itself.
It can be easily run on an workstation and doesn&#8217;t require servers, databases or
any other automated tools. With <strong class="program">ceph-deploy</strong>, it is really easy to set
up and take down a cluster. However, it is not a generic deployment tool. It is
a specific tool which is designed for those who want to get Ceph up and running
quickly with only the unavoidable initial configuration settings and without the
overhead of installing other tools like <tt class="docutils literal"><span class="pre">Chef</span></tt>, <tt class="docutils literal"><span class="pre">Puppet</span></tt> or <tt class="docutils literal"><span class="pre">Juju</span></tt>. Those
who want to customize security settings, partitions or directory locations and
want to set up a cluster following detailed manual steps, should use other tools
i.e, <tt class="docutils literal"><span class="pre">Chef</span></tt>, <tt class="docutils literal"><span class="pre">Puppet</span></tt>, <tt class="docutils literal"><span class="pre">Juju</span></tt> or <tt class="docutils literal"><span class="pre">Crowbar</span></tt>.</p>
<p>With <strong class="program">ceph-deploy</strong>, you can install Ceph packages on remote nodes,
create a cluster, add monitors, gather/forget keys, add OSDs and metadata
servers, configure admin hosts or take down the cluster.</p>
</div>
<div class="section" id="commands">
<h2>Commands<a class="headerlink" href="#commands" title="Permalink to this headline">¶</a></h2>
<div class="section" id="new">
<h3>new<a class="headerlink" href="#new" title="Permalink to this headline">¶</a></h3>
<p>Start deploying a new cluster and write a configuration file and keyring for it.
It tries to copy ssh keys from admin node to gain passwordless ssh to monitor
node(s), validates host IP, creates a cluster with a new initial monitor node or
nodes for monitor quorum, a ceph configuration file, a monitor secret keyring and
a log file for the new cluster. It populates the newly created Ceph configuration
file with <tt class="docutils literal"><span class="pre">fsid</span></tt> of cluster, hostnames and IP addresses of initial monitor
members under <tt class="docutils literal"><span class="pre">[global]</span></tt> section.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy new [MON][MON...]</pre>
</div>
<p>Here, [MON] is initial monitor hostname (short hostname i.e, <tt class="docutils literal"><span class="pre">hostname</span> <span class="pre">-s</span></tt>).</p>
<p>Other options like <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--no-ssh-copykey"><em class="xref std std-option">--no-ssh-copykey</em></a>, <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--fsid"><em class="xref std std-option">--fsid</em></a>,
<a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--cluster-network"><em class="xref std std-option">--cluster-network</em></a> and <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--public-network"><em class="xref std std-option">--public-network</em></a> can also be used with
this command.</p>
<p>If more than one network interface is used, <tt class="docutils literal"><span class="pre">public</span> <span class="pre">network</span></tt> setting has to be
added under <tt class="docutils literal"><span class="pre">[global]</span></tt> section of Ceph configuration file. If the public subnet
is given, <tt class="docutils literal"><span class="pre">new</span></tt> command will choose the one IP from the remote host that exists
within the subnet range. Public network can also be added at runtime using
<a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--public-network"><em class="xref std std-option">--public-network</em></a> option with the command as mentioned above.</p>
</div>
<div class="section" id="install">
<h3>install<a class="headerlink" href="#install" title="Permalink to this headline">¶</a></h3>
<p>Install Ceph packages on remote hosts. As a first step it installs
<tt class="docutils literal"><span class="pre">yum-plugin-priorities</span></tt> in admin and other nodes using passwordless ssh and sudo
so that Ceph packages from upstream repository get more priority. It then detects
the platform and distribution for the hosts and installs Ceph normally by
downloading distro compatible packages if adequate repo for Ceph is already added.
<tt class="docutils literal"><span class="pre">--release</span></tt> flag is used to get the latest release for installation. During
detection of platform and distribution before installation, if it finds the
<tt class="docutils literal"><span class="pre">distro.init</span></tt> to be <tt class="docutils literal"><span class="pre">sysvinit</span></tt> (Fedora, CentOS/RHEL etc), it doesn&#8217;t allow
installation with custom cluster name and uses the default name <tt class="docutils literal"><span class="pre">ceph</span></tt> for the
cluster.</p>
<p>If the user explicitly specifies a custom repo url with <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--repo-url"><em class="xref std std-option">--repo-url</em></a> for
installation, anything detected from the configuration will be overridden and
the custom repository location will be used for installation of Ceph packages.
If required, valid custom repositories are also detected and installed. In case
of installation from a custom repo a boolean is used to determine the logic
needed to proceed with a custom repo installation. A custom repo install helper
is used that goes through config checks to retrieve repos (and any extra repos
defined) and installs them. <tt class="docutils literal"><span class="pre">cd_conf</span></tt> is the object built from <tt class="docutils literal"><span class="pre">argparse</span></tt>
that holds the flags and information needed to determine what metadata from the
configuration is to be used.</p>
<p>A user can also opt to install only the repository without installing Ceph and
its dependencies by using <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--repo"><em class="xref std std-option">--repo</em></a> option.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy install [HOST][HOST...]</pre>
</div>
<p>Here, [HOST] is/are the host node(s) where Ceph is to be installed.</p>
<p>An option <tt class="docutils literal"><span class="pre">--release</span></tt> is used to install a release known as CODENAME
(default: firefly).</p>
<p>Other options like <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--testing"><em class="xref std std-option">--testing</em></a>, <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--dev"><em class="xref std std-option">--dev</em></a>, <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--adjust-repos"><em class="xref std std-option">--adjust-repos</em></a>,
<a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--no-adjust-repos"><em class="xref std std-option">--no-adjust-repos</em></a>, <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--repo"><em class="xref std std-option">--repo</em></a>, <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--local-mirror"><em class="xref std std-option">--local-mirror</em></a>,
<a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--repo-url"><em class="xref std std-option">--repo-url</em></a> and <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--gpg-url"><em class="xref std std-option">--gpg-url</em></a> can also be used with this command.</p>
</div>
<div class="section" id="mds">
<h3>mds<a class="headerlink" href="#mds" title="Permalink to this headline">¶</a></h3>
<p>Deploy Ceph mds on remote hosts. A metadata server is needed to use CephFS and
the <tt class="docutils literal"><span class="pre">mds</span></tt> command is used to create one on the desired host node. It uses the
subcommand <tt class="docutils literal"><span class="pre">create</span></tt> to do so. <tt class="docutils literal"><span class="pre">create</span></tt> first gets the hostname and distro
information of the desired mds host. It then tries to read the <tt class="docutils literal"><span class="pre">bootstrap-mds</span></tt>
key for the cluster and deploy it in the desired host. The key generally has a
format of <tt class="docutils literal"><span class="pre">{cluster}.bootstrap-mds.keyring</span></tt>. If it doesn&#8217;t finds a keyring,
it runs <tt class="docutils literal"><span class="pre">gatherkeys</span></tt> to get the keyring. It then creates a mds on the desired
host under the path <tt class="docutils literal"><span class="pre">/var/lib/ceph/mds/</span></tt> in <tt class="docutils literal"><span class="pre">/var/lib/ceph/mds/{cluster}-{name}</span></tt>
format and a bootstrap keyring under <tt class="docutils literal"><span class="pre">/var/lib/ceph/bootstrap-mds/</span></tt> in
<tt class="docutils literal"><span class="pre">/var/lib/ceph/bootstrap-mds/{cluster}.keyring</span></tt> format. It then runs appropriate
commands based on <tt class="docutils literal"><span class="pre">distro.init</span></tt> to start the <tt class="docutils literal"><span class="pre">mds</span></tt>. To remove the mds,
subcommand <tt class="docutils literal"><span class="pre">destroy</span></tt> is used.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy mds create [HOST[:DAEMON-NAME]] [HOST[:DAEMON-NAME]...]

ceph-deploy mds destroy [HOST[:DAEMON-NAME]] [HOST[:DAEMON-NAME]...]</pre>
</div>
<p>The [DAEMON-NAME] is optional.</p>
</div>
<div class="section" id="mon">
<h3>mon<a class="headerlink" href="#mon" title="Permalink to this headline">¶</a></h3>
<p>Deploy Ceph monitor on remote hosts. <tt class="docutils literal"><span class="pre">mon</span></tt> makes use of certain subcommands
to deploy Ceph monitors on other nodes.</p>
<p>Subcommand <tt class="docutils literal"><span class="pre">create-initial</span></tt> deploys for monitors defined in
<tt class="docutils literal"><span class="pre">mon</span> <span class="pre">initial</span> <span class="pre">members</span></tt> under <tt class="docutils literal"><span class="pre">[global]</span></tt> section in Ceph configuration file,
wait until they form quorum and then gatherkeys, reporting the monitor status
along the process. If monitors don&#8217;t form quorum the command will eventually
time out.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy mon create-initial</pre>
</div>
<p>Subcommand <tt class="docutils literal"><span class="pre">create</span></tt> is used to deploy Ceph monitors by explicitly specifying
the hosts which are desired to be made monitors. If no hosts are specified it
will default to use the <tt class="docutils literal"><span class="pre">mon</span> <span class="pre">initial</span> <span class="pre">members</span></tt> defined under <tt class="docutils literal"><span class="pre">[global]</span></tt>
section of Ceph configuration file. <tt class="docutils literal"><span class="pre">create</span></tt> first detects platform and distro
for desired hosts and checks if hostname is compatible for deployment. It then
uses the monitor keyring initially created using <tt class="docutils literal"><span class="pre">new</span></tt> command and deploys the
monitor in desired host. If multiple hosts were specified during <tt class="docutils literal"><span class="pre">new</span></tt> command
i.e, if there are multiple hosts in <tt class="docutils literal"><span class="pre">mon</span> <span class="pre">initial</span> <span class="pre">members</span></tt> and multiple keyrings
were created then a concatenated keyring is used for deployment of monitors. In
this process a keyring parser is used which looks for <tt class="docutils literal"><span class="pre">[entity]</span></tt> sections in
monitor keyrings and returns a list of those sections. A helper is then used to
collect all keyrings into a single blob that will be used to inject it to monitors
with <em class="xref std std-option">--mkfs</em> on remote nodes. All keyring files are concatenated to be
in a directory ending with <tt class="docutils literal"><span class="pre">.keyring</span></tt>. During this process the helper uses list
of sections returned by keyring parser to check if an entity is already present
in a keyring and if not, adds it. The concatenated keyring is used for deployment
of monitors to desired multiple hosts.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy mon create [HOST] [HOST...]</pre>
</div>
<p>Here, [HOST] is hostname of desired monitor host(s).</p>
<p>Subcommand <tt class="docutils literal"><span class="pre">add</span></tt> is used to add a monitor to an existing cluster. It first
detects platform and distro for desired host and checks if hostname is compatible
for deployment. It then uses the monitor keyring, ensures configuration for new
monitor host and adds the monitor to the cluster. If the section for the monitor
exists and defines a mon addr that will be used, otherwise it will fallback by
resolving the hostname to an IP. If <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--address"><em class="xref std std-option">--address</em></a> is used it will override
all other options. After adding the monitor to the cluster, it gives it some time
to start. It then looks for any monitor errors and checks monitor status. Monitor
errors arise if the monitor is not added in <tt class="docutils literal"><span class="pre">mon</span> <span class="pre">initial</span> <span class="pre">members</span></tt>, if it doesn&#8217;t
exist in <tt class="docutils literal"><span class="pre">monmap</span></tt> and if neither <tt class="docutils literal"><span class="pre">public_addr</span></tt> nor <tt class="docutils literal"><span class="pre">public_network</span></tt> keys
were defined for monitors. Under such conditions, monitors may not be able to
form quorum. Monitor status tells if the monitor is up and running normally. The
status is checked by running <tt class="docutils literal"><span class="pre">ceph</span> <span class="pre">daemon</span> <span class="pre">mon.hostname</span> <span class="pre">mon_status</span></tt> on remote
end which provides the output and returns a boolean status of what is going on.
<tt class="docutils literal"><span class="pre">False</span></tt> means a monitor that is not fine even if it is up and running, while
<tt class="docutils literal"><span class="pre">True</span></tt> means the monitor is up and running correctly.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy mon add [HOST]

ceph-deploy mon add [HOST] --address [IP]</pre>
</div>
<p>Here, [HOST] is the hostname and [IP] is the IP address of the desired monitor
node.</p>
<p>Subcommand <tt class="docutils literal"><span class="pre">destroy</span></tt> is used to completely remove monitors on remote hosts.
It takes hostnames as arguments. It stops the monitor, verifies if <tt class="docutils literal"><span class="pre">ceph-mon</span></tt>
daemon really stopped, creates an archive directory <tt class="docutils literal"><span class="pre">mon-remove</span></tt> under
<tt class="docutils literal"><span class="pre">/var/lib/ceph/</span></tt>, archives old monitor directory in
<tt class="docutils literal"><span class="pre">{cluster}-{hostname}-{stamp}</span></tt> format in it and removes the monitor from
cluster by running <tt class="docutils literal"><span class="pre">ceph</span> <span class="pre">remove...</span></tt> command.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy mon destroy [HOST]</pre>
</div>
<p>Here, [HOST] is hostname of monitor that is to be removed.</p>
</div>
<div class="section" id="gatherkeys">
<h3>gatherkeys<a class="headerlink" href="#gatherkeys" title="Permalink to this headline">¶</a></h3>
<p>Gather authentication keys for provisioning new nodes. It takes hostnames as
arguments. It checks for and fetches <tt class="docutils literal"><span class="pre">client.admin</span></tt> keyring, monitor keyring
and <tt class="docutils literal"><span class="pre">bootstrap-mds/bootstrap-osd</span></tt> keyring from monitor host. These
authentication keys are used when new <tt class="docutils literal"><span class="pre">monitors/OSDs/MDS</span></tt> are added to the
cluster.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy gatherkeys [HOST] [HOST...]</pre>
</div>
<p>Here, [HOST] is hostname of the monitor from where keys are to be pulled.</p>
</div>
<div class="section" id="disk">
<h3>disk<a class="headerlink" href="#disk" title="Permalink to this headline">¶</a></h3>
<p>Manage disks on a remote host. It actually triggers the <tt class="docutils literal"><span class="pre">ceph-disk</span></tt> utility
and it&#8217;s subcommands to manage disks.</p>
<p>Subcommand <tt class="docutils literal"><span class="pre">list</span></tt> lists disk partitions and Ceph OSDs.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy disk list [HOST:[DISK]]</pre>
</div>
<p>Here, [HOST] is hostname of the node and [DISK] is disk name or path.</p>
<p>Subcommand <tt class="docutils literal"><span class="pre">prepare</span></tt> prepares a directory, disk or drive for a Ceph OSD. It
creates a GPT partition, marks the partition with Ceph type uuid, creates a
file system, marks the file system as ready for Ceph consumption, uses entire
partition and adds a new partition to the journal disk.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy disk prepare [HOST:[DISK]]</pre>
</div>
<p>Here, [HOST] is hostname of the node and [DISK] is disk name or path.</p>
<p>Subcommand <tt class="docutils literal"><span class="pre">activate</span></tt> activates the Ceph OSD. It mounts the volume in a
temporary location, allocates an OSD id (if needed), remounts in the correct
location <tt class="docutils literal"><span class="pre">/var/lib/ceph/osd/$cluster-$id</span></tt> and starts <tt class="docutils literal"><span class="pre">ceph-osd</span></tt>. It is
triggered by <tt class="docutils literal"><span class="pre">udev</span></tt> when it sees the OSD GPT partition type or on ceph service
start with <tt class="docutils literal"><span class="pre">ceph</span> <span class="pre">disk</span> <span class="pre">activate-all</span></tt>.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy disk activate [HOST:[DISK]]</pre>
</div>
<p>Here, [HOST] is hostname of the node and [DISK] is disk name or path.</p>
<p>Subcommand <tt class="docutils literal"><span class="pre">zap</span></tt> zaps/erases/destroys a device&#8217;s partition table and contents.
It actually uses <tt class="docutils literal"><span class="pre">sgdisk</span></tt> and it&#8217;s option <tt class="docutils literal"><span class="pre">--zap-all</span></tt> to destroy both GPT and
MBR data structures so that the disk becomes suitable for repartitioning.
<tt class="docutils literal"><span class="pre">sgdisk</span></tt> then uses <tt class="docutils literal"><span class="pre">--mbrtogpt</span></tt> to convert the MBR or BSD disklabel disk to a
GPT disk. The <tt class="docutils literal"><span class="pre">prepare</span></tt> subcommand can now be executed which will create a new
GPT partition.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy disk zap [HOST:[DISK]]</pre>
</div>
<p>Here, [HOST] is hostname of the node and [DISK] is disk name or path.</p>
</div>
<div class="section" id="osd">
<h3>osd<a class="headerlink" href="#osd" title="Permalink to this headline">¶</a></h3>
<p>Manage OSDs by preparing data disk on remote host. <tt class="docutils literal"><span class="pre">osd</span></tt> makes use of certain
subcommands for managing OSDs.</p>
<p>Subcommand <tt class="docutils literal"><span class="pre">prepare</span></tt> prepares a directory, disk or drive for a Ceph OSD. It
first checks against multiple OSDs getting created and warns about the
possibility of more than the recommended which would cause issues with max
allowed PIDs in a system. It then reads the bootstrap-osd key for the cluster or
writes the bootstrap key if not found. It then uses <strong class="program">ceph-disk</strong>
utility&#8217;s <tt class="docutils literal"><span class="pre">prepare</span></tt> subcommand to prepare the disk, journal and deploy the OSD
on the desired host. Once prepared, it gives some time to the OSD to settle and
checks for any possible errors and if found, reports to the user.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy osd prepare HOST:DISK[:JOURNAL] [HOST:DISK[:JOURNAL]...]</pre>
</div>
<p>Subcommand <tt class="docutils literal"><span class="pre">activate</span></tt> activates the OSD prepared using <tt class="docutils literal"><span class="pre">prepare</span></tt> subcommand.
It actually uses <strong class="program">ceph-disk</strong> utility&#8217;s <tt class="docutils literal"><span class="pre">activate</span></tt> subcommand with
appropriate init type based on distro to activate the OSD. Once activated, it
gives some time to the OSD to start and checks for any possible errors and if
found, reports to the user. It checks the status of the prepared OSD, checks the
OSD tree and makes sure the OSDs are up and in.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy osd activate HOST:DISK[:JOURNAL] [HOST:DISK[:JOURNAL]...]</pre>
</div>
<p>Subcommand <tt class="docutils literal"><span class="pre">create</span></tt> uses <tt class="docutils literal"><span class="pre">prepare</span></tt> and <tt class="docutils literal"><span class="pre">activate</span></tt> subcommands to create an
OSD.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy osd create HOST:DISK[:JOURNAL] [HOST:DISK[:JOURNAL]...]</pre>
</div>
<p>Subcommand <tt class="docutils literal"><span class="pre">list</span></tt> lists disk partitions, Ceph OSDs and prints OSD metadata.
It gets the osd tree from a monitor host, uses the <tt class="docutils literal"><span class="pre">ceph-disk-list</span></tt> output
and gets the mount point by matching the line where the partition mentions
the OSD name, reads metadata from files, checks if a journal path exists,
if the OSD is in a OSD tree and prints the OSD metadata.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy osd list HOST:DISK[:JOURNAL] [HOST:DISK[:JOURNAL]...]</pre>
</div>
</div>
<div class="section" id="admin">
<h3>admin<a class="headerlink" href="#admin" title="Permalink to this headline">¶</a></h3>
<p>Push configuration and <tt class="docutils literal"><span class="pre">client.admin</span></tt> key to a remote host. It takes
the <tt class="docutils literal"><span class="pre">{cluster}.client.admin.keyring</span></tt> from admin node and writes it under
<tt class="docutils literal"><span class="pre">/etc/ceph</span></tt> directory of desired node.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy admin [HOST] [HOST...]</pre>
</div>
<p>Here, [HOST] is desired host to be configured for Ceph administration.</p>
</div>
<div class="section" id="config">
<h3>config<a class="headerlink" href="#config" title="Permalink to this headline">¶</a></h3>
<p>Push/pull configuration file to/from a remote host. It uses <tt class="docutils literal"><span class="pre">push</span></tt> subcommand
to takes the configuration file from admin host and write it to remote host under
<tt class="docutils literal"><span class="pre">/etc/ceph</span></tt> directory. It uses <tt class="docutils literal"><span class="pre">pull</span></tt> subcommand to do the opposite i.e, pull
the configuration file under <tt class="docutils literal"><span class="pre">/etc/ceph</span></tt> directory of remote host to admin node.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy push [HOST] [HOST...]

ceph-deploy pull [HOST] [HOST...]</pre>
</div>
<p>Here, [HOST] is the hostname of the node where config file will be pushed to or
pulled from.</p>
</div>
<div class="section" id="uninstall">
<h3>uninstall<a class="headerlink" href="#uninstall" title="Permalink to this headline">¶</a></h3>
<p>Remove Ceph packages from remote hosts. It detects the platform and distro of
selected host and uninstalls Ceph packages from it. However, some dependencies
like <tt class="docutils literal"><span class="pre">librbd1</span></tt> and <tt class="docutils literal"><span class="pre">librados2</span></tt> will not be removed because they can cause
issues with <tt class="docutils literal"><span class="pre">qemu-kvm</span></tt>.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy uninstall [HOST] [HOST...]</pre>
</div>
<p>Here, [HOST] is hostname of the node from where Ceph will be uninstalled.</p>
</div>
<div class="section" id="purge">
<h3>purge<a class="headerlink" href="#purge" title="Permalink to this headline">¶</a></h3>
<p>Remove Ceph packages from remote hosts and purge all data. It detects the
platform and distro of selected host, uninstalls Ceph packages and purges all
data. However, some dependencies like <tt class="docutils literal"><span class="pre">librbd1</span></tt> and <tt class="docutils literal"><span class="pre">librados2</span></tt> will not be
removed because they can cause issues with <tt class="docutils literal"><span class="pre">qemu-kvm</span></tt>.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy purge [HOST] [HOST...]</pre>
</div>
<p>Here, [HOST] is hostname of the node from where Ceph will be purged.</p>
</div>
<div class="section" id="purgedata">
<h3>purgedata<a class="headerlink" href="#purgedata" title="Permalink to this headline">¶</a></h3>
<p>Purge (delete, destroy, discard, shred) any Ceph data from <tt class="docutils literal"><span class="pre">/var/lib/ceph</span></tt>.
Once it detects the platform and distro of desired host, it first checks if Ceph
is still installed on the selected host and if installed, it won&#8217;t purge data
from it. If Ceph is already uninstalled from the host, it tries to remove the
contents of <tt class="docutils literal"><span class="pre">/var/lib/ceph</span></tt>. If it fails then probably OSDs are still mounted
and needs to be unmounted to continue. It unmount the OSDs and tries to remove
the contents of <tt class="docutils literal"><span class="pre">/var/lib/ceph</span></tt> again and checks for errors. It also removes
contents of <tt class="docutils literal"><span class="pre">/etc/ceph</span></tt>. Once all steps are successfully completed, all the
Ceph data from the selected host are removed.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy purgedata [HOST] [HOST...]</pre>
</div>
<p>Here, [HOST] is hostname of the node from where Ceph data will be purged.</p>
</div>
<div class="section" id="forgetkeys">
<h3>forgetkeys<a class="headerlink" href="#forgetkeys" title="Permalink to this headline">¶</a></h3>
<p>Remove authentication keys from the local directory. It removes all the
authentication keys i.e, monitor keyring, client.admin keyring, bootstrap-osd
and bootstrap-mds keyring from the node.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy forgetkeys</pre>
</div>
</div>
<div class="section" id="pkg">
<h3>pkg<a class="headerlink" href="#pkg" title="Permalink to this headline">¶</a></h3>
<p>Manage packages on remote hosts. It is used for installing or removing packages
from remote hosts. The package names for installation or removal are to be
specified after the command. Two options <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--install"><em class="xref std std-option">--install</em></a> and
<a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--remove"><em class="xref std std-option">--remove</em></a> are used for this purpose.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy pkg --install [PKGs] [HOST] [HOST...]

ceph-deploy pkg --remove [PKGs] [HOST] [HOST...]</pre>
</div>
<p>Here, [PKGs] is comma-separated package names and [HOST] is hostname of the
remote node where packages are to be installed or removed from.</p>
</div>
<div class="section" id="calamari">
<h3>calamari<a class="headerlink" href="#calamari" title="Permalink to this headline">¶</a></h3>
<p>Install and configure Calamari nodes. It first checks if distro is supported
for Calamari installation by ceph-deploy. An argument <tt class="docutils literal"><span class="pre">connect</span></tt> is used for
installation and configuration. It checks for <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> configuration
file (cd_conf) and Calamari release repo or <tt class="docutils literal"><span class="pre">calamari-minion</span></tt> repo. It relies
on default for repo installation as it doesn&#8217;t install Ceph unless specified
otherwise. <tt class="docutils literal"><span class="pre">options</span></tt> dictionary is also defined because <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt>
pops items internally which causes issues when those items are needed to be
available for every host. If the distro is Debian/Ubuntu, it is ensured that
proxy is disabled for <tt class="docutils literal"><span class="pre">calamari-minion</span></tt> repo. <tt class="docutils literal"><span class="pre">calamari-minion</span></tt> package is
then installed and custom repository files are added. minion config  is placed
prior to installation so that it is present when the minion first starts.
config directory, calamari salt config are created and salt-minion package
is installed. If the distro is Redhat/CentOS, the salt-minion service needs to
be started.</p>
<p>Usage:</p>
<div class="highlight-python"><pre>ceph-deploy calamari {connect} [HOST] [HOST...]</pre>
</div>
<p>Here, [HOST] is the hostname where Calamari is to be installed.</p>
<p>An option <tt class="docutils literal"><span class="pre">--release</span></tt> can be used to use a given release from repositories
defined in <strong class="program">ceph-deploy</strong>&#8216;s configuration. Defaults to <tt class="docutils literal"><span class="pre">calamari-minion</span></tt>.</p>
<p>Another option <a class="reference internal" href="../../../../man/8/ceph-deploy/#cmdoption-ceph-deploy--master"><em class="xref std std-option">--master</em></a> can also be used with this command.</p>
</div>
</div>
<div class="section" id="options">
<h2>Options<a class="headerlink" href="#options" title="Permalink to this headline">¶</a></h2>
<dl class="option">
<dt id="cmdoption-ceph-deploy--version">
<tt class="descname">--version</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--version" title="Permalink to this definition">¶</a></dt>
<dd><p>The current installed version of <strong class="program">ceph-deploy</strong>.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--username">
<tt class="descname">--username</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--username" title="Permalink to this definition">¶</a></dt>
<dd><p>The username to connect to the remote host.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--overwrite-conf">
<tt class="descname">--overwrite-conf</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--overwrite-conf" title="Permalink to this definition">¶</a></dt>
<dd><p>Overwrite an existing conf file on remote host (if present).</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--cluster">
<tt class="descname">--cluster</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--cluster" title="Permalink to this definition">¶</a></dt>
<dd><p>Name of the cluster.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--ceph-conf">
<tt class="descname">--ceph-conf</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--ceph-conf" title="Permalink to this definition">¶</a></dt>
<dd><p>Use (or reuse) a given <tt class="docutils literal"><span class="pre">ceph.conf</span></tt> file.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--no-ssh-copykey">
<tt class="descname">--no-ssh-copykey</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--no-ssh-copykey" title="Permalink to this definition">¶</a></dt>
<dd><p>Do not attempt to copy ssh keys.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--fsid">
<tt class="descname">--fsid</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--fsid" title="Permalink to this definition">¶</a></dt>
<dd><p>Provide an alternate FSID for <tt class="docutils literal"><span class="pre">ceph.conf</span></tt> generation.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--cluster-network">
<tt class="descname">--cluster-network</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--cluster-network" title="Permalink to this definition">¶</a></dt>
<dd><p>Specify the (internal) cluster network.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--public-network">
<tt class="descname">--public-network</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--public-network" title="Permalink to this definition">¶</a></dt>
<dd><p>Specify the public network for a cluster.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--testing">
<tt class="descname">--testing</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--testing" title="Permalink to this definition">¶</a></dt>
<dd><p>Install the latest development release.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--dev">
<tt class="descname">--dev</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--dev" title="Permalink to this definition">¶</a></dt>
<dd><p>Install a bleeding edge built from Git branch or tag (default: master).</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--adjust-repos">
<tt class="descname">--adjust-repos</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--adjust-repos" title="Permalink to this definition">¶</a></dt>
<dd><p>Install packages modifying source repos.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--no-adjust-repos">
<tt class="descname">--no-adjust-repos</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--no-adjust-repos" title="Permalink to this definition">¶</a></dt>
<dd><p>Install packages without modifying source repos.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--repo">
<tt class="descname">--repo</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--repo" title="Permalink to this definition">¶</a></dt>
<dd><p>Install repo files only (skips package installation).</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--local-mirror">
<tt class="descname">--local-mirror</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--local-mirror" title="Permalink to this definition">¶</a></dt>
<dd><p>Fetch packages and push them to hosts for a local repo mirror.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--repo-url">
<tt class="descname">--repo-url</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--repo-url" title="Permalink to this definition">¶</a></dt>
<dd><p>Specify a repo url that mirrors/contains Ceph packages.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--gpg-url">
<tt class="descname">--gpg-url</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--gpg-url" title="Permalink to this definition">¶</a></dt>
<dd><p>Specify a GPG key url to be used with custom repos (defaults to ceph.com).</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--address">
<tt class="descname">--address</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--address" title="Permalink to this definition">¶</a></dt>
<dd><p>IP address of the host node to be added to the cluster.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--keyrings">
<tt class="descname">--keyrings</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--keyrings" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenate multiple keyrings to be seeded on new monitors.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--zap-disk">
<tt class="descname">--zap-disk</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--zap-disk" title="Permalink to this definition">¶</a></dt>
<dd><p>Destroy the partition table and content of a disk.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--fs-type">
<tt class="descname">--fs-type</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--fs-type" title="Permalink to this definition">¶</a></dt>
<dd><p>Filesystem to use to format disk <tt class="docutils literal"><span class="pre">(xfs,</span> <span class="pre">btrfs</span> <span class="pre">or</span> <span class="pre">ext4)</span></tt>.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--dmcrypt">
<tt class="descname">--dmcrypt</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--dmcrypt" title="Permalink to this definition">¶</a></dt>
<dd><p>Encrypt [data-path] and/or journal devices with <tt class="docutils literal"><span class="pre">dm-crypt</span></tt>.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--dmcrypt-key-dir">
<tt class="descname">--dmcrypt-key-dir</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--dmcrypt-key-dir" title="Permalink to this definition">¶</a></dt>
<dd><p>Directory where <tt class="docutils literal"><span class="pre">dm-crypt</span></tt> keys are stored.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--install">
<tt class="descname">--install</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--install" title="Permalink to this definition">¶</a></dt>
<dd><p>Comma-separated package(s) to install on remote hosts.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--remove">
<tt class="descname">--remove</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--remove" title="Permalink to this definition">¶</a></dt>
<dd><p>Comma-separated package(s) to remove from remote hosts.</p>
</dd></dl>

<dl class="option">
<dt id="cmdoption-ceph-deploy--master">
<tt class="descname">--master</tt><tt class="descclassname"></tt><a class="headerlink" href="#cmdoption-ceph-deploy--master" title="Permalink to this definition">¶</a></dt>
<dd><p>The domain for the Calamari master server.</p>
</dd></dl>

</div>
<div class="section" id="availability">
<h2>Availability<a class="headerlink" href="#availability" title="Permalink to this headline">¶</a></h2>
<p><strong class="program">ceph-deploy</strong> is part of Ceph, a massively scalable, open-source, distributed storage system. Please refer to
the documentation at <a class="reference external" href="http://ceph.com/ceph-deploy/docs">http://ceph.com/ceph-deploy/docs</a> for more information.</p>
</div>
<div class="section" id="see-also">
<h2>See also<a class="headerlink" href="#see-also" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="../ceph-mon/"><em>ceph-mon</em></a>(8),
<a class="reference internal" href="../ceph-osd/"><em>ceph-osd</em></a>(8),
<a class="reference internal" href="../ceph-disk/"><em>ceph-disk</em></a>(8),
<a class="reference internal" href="../ceph-mds/"><em>ceph-mds</em></a>(8)</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../../">
              <img class="logo" src="../../../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../../../">Table Of Contents</a></h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../start/intro/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../start/">安装（快速）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../install/">安装（手动）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rbd/rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dev/">开发文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../release-notes/">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../releases/">发布时间表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../glossary/">Ceph 术语</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../../../../">Ceph Documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2010-2014, Inktank Storage, Inc. and contributors. Licensed under Creative Commons BY-SA.
    </div>
  </body>
</html>
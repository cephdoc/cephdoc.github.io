
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Storage Cluster Quick Start &mdash; Ceph Documentation</title>
    
    <link rel="stylesheet" href="../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     'dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="top" title="Ceph Documentation" href="../../../" />
    <script type="text/javascript" src="http://ayni.ceph.com/public/js/ceph.js"></script>

  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../../../">Ceph Documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="storage-cluster-quick-start">
<h1>Storage Cluster Quick Start<a class="headerlink" href="#storage-cluster-quick-start" title="Permalink to this headline">¶</a></h1>
<p>If you haven&#8217;t completed your <a class="reference external" href="../quick-start-preflight">Preflight Checklist</a>, do that first. This
<strong>Quick Start</strong> sets up a <a class="reference internal" href="../../../glossary/#term-ceph-storage-cluster"><em class="xref std std-term">Ceph Storage Cluster</em></a> using <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt>
on your admin node. Create a three Ceph Node cluster so you can
explore Ceph functionality.</p>
<p class="ditaa">
<img src="../../../_images/ditaa-5d5cab6fc315585e5057a743b5af7946fba43b24.png"/>
</p>
<p>As a first exercise, create a Ceph Storage Cluster with one Ceph Monitor and two
Ceph OSD Daemons. Once the cluster reaches a <tt class="docutils literal"><span class="pre">active</span> <span class="pre">+</span> <span class="pre">clean</span></tt> state, expand it
by adding a third Ceph OSD Daemon, a Metadata Server and two more Ceph Monitors.
For best results, create a directory on your admin node node for maintaining the
configuration files and keys that <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> generates for your cluster.</p>
<div class="highlight-python"><pre>mkdir my-cluster
cd my-cluster</pre>
</div>
<p>The <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> utility will output files to the current directory. Ensure you
are in this directory when executing <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt>.</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">Do not call <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> with <tt class="docutils literal"><span class="pre">sudo</span></tt> or run it as <tt class="docutils literal"><span class="pre">root</span></tt>
if you are logged in as a different user, because it will not issue <tt class="docutils literal"><span class="pre">sudo</span></tt>
commands needed on the remote host.</p>
</div>
<div class="topic">
<p class="topic-title first">Disable <tt class="docutils literal"><span class="pre">requiretty</span></tt></p>
<p>On some distributions (e.g., CentOS), you may receive an error while trying
to execute <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> commands. If <tt class="docutils literal"><span class="pre">requiretty</span></tt> is set
by default, disable it by executing <tt class="docutils literal"><span class="pre">sudo</span> <span class="pre">visudo</span></tt> and locate the
<tt class="docutils literal"><span class="pre">Defaults</span> <span class="pre">requiretty</span></tt> setting. Change it to <tt class="docutils literal"><span class="pre">Defaults:ceph</span> <span class="pre">!requiretty</span></tt> to
ensure that <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> can connect using the <tt class="docutils literal"><span class="pre">ceph</span></tt> user and execute
commands with <tt class="docutils literal"><span class="pre">sudo</span></tt>.</p>
</div>
<div class="section" id="create-a-cluster">
<h2>Create a Cluster<a class="headerlink" href="#create-a-cluster" title="Permalink to this headline">¶</a></h2>
<p>If at any point you run into trouble and you want to start over, execute
the following to purge the configuration:</p>
<div class="highlight-python"><pre>ceph-deploy purgedata {ceph-node} [{ceph-node}]
ceph-deploy forgetkeys</pre>
</div>
<p>To purge the Ceph packages too, you may also execute:</p>
<div class="highlight-python"><pre>ceph-deploy purge {ceph-node} [{ceph-node}]</pre>
</div>
<p>If you execute <tt class="docutils literal"><span class="pre">purge</span></tt>, you must re-install Ceph.</p>
<p>On your admin node from the directory you created for holding your
configuration details, perform the following steps using <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt>.</p>
<ol class="arabic">
<li><p class="first">Create the cluster.</p>
<div class="highlight-python"><pre>ceph-deploy new {initial-monitor-node(s)}</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph-deploy new node1</pre>
</div>
<p>Check the output of <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> with <tt class="docutils literal"><span class="pre">ls</span></tt> and <tt class="docutils literal"><span class="pre">cat</span></tt> in the current
directory. You should see a Ceph configuration file, a monitor secret
keyring, and a log file for the new cluster.  See <a class="reference external" href="../../rados/deployment/ceph-deploy-new">ceph-deploy new -h</a>
for additional details.</p>
</li>
<li><p class="first">Change the default number of replicas in the Ceph configuration file from
<tt class="docutils literal"><span class="pre">3</span></tt> to <tt class="docutils literal"><span class="pre">2</span></tt> so that Ceph can achieve an <tt class="docutils literal"><span class="pre">active</span> <span class="pre">+</span> <span class="pre">clean</span></tt> state with
just two Ceph OSDs. Add the following line under the <tt class="docutils literal"><span class="pre">[global]</span></tt> section:</p>
<div class="highlight-python"><pre>osd pool default size = 2</pre>
</div>
</li>
<li><p class="first">If you have more than one network interface, add the <tt class="docutils literal"><span class="pre">public</span> <span class="pre">network</span></tt>
setting under the <tt class="docutils literal"><span class="pre">[global]</span></tt> section of your Ceph configuration file.
See the <a class="reference external" href="../../rados/configuration/network-config-ref">Network Configuration Reference</a> for details.</p>
<div class="highlight-python"><pre>public network = {ip-address}/{netmask}</pre>
</div>
</li>
<li><p class="first">Install Ceph.</p>
<div class="highlight-python"><pre>ceph-deploy install {ceph-node}[{ceph-node} ...]</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph-deploy install admin-node node1 node2 node3</pre>
</div>
<p>The <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> utility will install Ceph on each node.
<strong>NOTE</strong>: If you use <tt class="docutils literal"><span class="pre">ceph-deploy</span> <span class="pre">purge</span></tt>, you must re-execute this step
to re-install Ceph.</p>
</li>
<li><p class="first">Add the initial monitor(s) and gather the keys:</p>
<div class="highlight-python"><pre>ceph-deploy mon create-initial</pre>
</div>
<p>Once you complete the process, your local directory should have the following
keyrings:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">{cluster-name}.client.admin.keyring</span></tt></li>
<li><tt class="docutils literal"><span class="pre">{cluster-name}.bootstrap-osd.keyring</span></tt></li>
<li><tt class="docutils literal"><span class="pre">{cluster-name}.bootstrap-mds.keyring</span></tt></li>
<li><tt class="docutils literal"><span class="pre">{cluster-name}.bootstrap-rgw.keyring</span></tt></li>
</ul>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The bootstrap-rgw keyring is only created during installation of clusters
running Hammer or newer</p>
</div>
<ol class="arabic">
<li><p class="first">Add two OSDs. For fast setup, this quick start uses a directory rather
than an entire disk per Ceph OSD Daemon. See <a class="reference external" href="../../rados/deployment/ceph-deploy-osd">ceph-deploy osd</a> for
details on using separate disks/partitions for OSDs and journals.
Login to the Ceph Nodes and create a directory for
the Ceph OSD Daemon.</p>
<div class="highlight-python"><pre>ssh node2
sudo mkdir /var/local/osd0
exit

ssh node3
sudo mkdir /var/local/osd1
exit</pre>
</div>
<p>Then, from your admin node, use <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> to prepare the OSDs.</p>
<div class="highlight-python"><pre>ceph-deploy osd prepare {ceph-node}:/path/to/directory</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph-deploy osd prepare node2:/var/local/osd0 node3:/var/local/osd1</pre>
</div>
<p>Finally, activate the OSDs.</p>
<div class="highlight-python"><pre>ceph-deploy osd activate {ceph-node}:/path/to/directory</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph-deploy osd activate node2:/var/local/osd0 node3:/var/local/osd1</pre>
</div>
</li>
<li><p class="first">Use <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> to copy the configuration file and admin key to
your admin node and your Ceph Nodes so that you can use the <tt class="docutils literal"><span class="pre">ceph</span></tt>
CLI without having to specify the monitor address and
<tt class="docutils literal"><span class="pre">ceph.client.admin.keyring</span></tt> each time you execute a command.</p>
<div class="highlight-python"><pre>ceph-deploy admin {admin-node} {ceph-node}</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph-deploy admin admin-node node1 node2 node3</pre>
</div>
<p>When <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> is talking to the local admin host (<tt class="docutils literal"><span class="pre">admin-node</span></tt>),
it must be reachable by its hostname. If necessary, modify <tt class="docutils literal"><span class="pre">/etc/hosts</span></tt>
to add the name of the admin host.</p>
</li>
<li><p class="first">Ensure that you have the correct permissions for the
<tt class="docutils literal"><span class="pre">ceph.client.admin.keyring</span></tt>.</p>
<div class="highlight-python"><pre>sudo chmod +r /etc/ceph/ceph.client.admin.keyring</pre>
</div>
</li>
<li><p class="first">Check your cluster&#8217;s health.</p>
<div class="highlight-python"><pre>ceph health</pre>
</div>
<p>Your cluster should return an <tt class="docutils literal"><span class="pre">active</span> <span class="pre">+</span> <span class="pre">clean</span></tt> state when it
has finished peering.</p>
</li>
</ol>
</div>
<div class="section" id="operating-your-cluster">
<h2>Operating Your Cluster<a class="headerlink" href="#operating-your-cluster" title="Permalink to this headline">¶</a></h2>
<p>Deploying a Ceph cluster with <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> automatically starts the cluster.
To operate the cluster daemons with Debian/Ubuntu distributions, see
<a class="reference external" href="../../rados/operations/operating#running-ceph-with-upstart">Running Ceph with Upstart</a>.  To operate the cluster daemons with CentOS,
Red Hat, Fedora, and SLES distributions, see <a class="reference external" href="../../rados/operations/operating#running-ceph-with-sysvinit">Running Ceph with sysvinit</a>.</p>
<p>To learn more about peering and cluster health, see <a class="reference external" href="../../rados/operations/monitoring">Monitoring a Cluster</a>.
To learn more about Ceph OSD Daemon and placement group health, see
<a class="reference external" href="../../rados/operations/monitoring-osd-pg">Monitoring OSDs and PGs</a>. To learn more about managing users, see
<a class="reference external" href="../../rados/operations/user-management">User Management</a>.</p>
<p>Once you deploy a Ceph cluster, you can try out some of the administration
functionality, the <tt class="docutils literal"><span class="pre">rados</span></tt> object store command line, and then proceed to
Quick Start guides for Ceph Block Device, Ceph Filesystem, and the Ceph Object
Gateway.</p>
</div>
<div class="section" id="expanding-your-cluster">
<h2>Expanding Your Cluster<a class="headerlink" href="#expanding-your-cluster" title="Permalink to this headline">¶</a></h2>
<p>Once you have a basic cluster up and running, the next step is to expand
cluster. Add a Ceph OSD Daemon and a Ceph Metadata Server to <tt class="docutils literal"><span class="pre">node1</span></tt>.
Then add a Ceph Monitor to <tt class="docutils literal"><span class="pre">node2</span></tt> and  <tt class="docutils literal"><span class="pre">node3</span></tt> to establish a
quorum of Ceph Monitors.</p>
<p class="ditaa">
<img src="../../../_images/ditaa-96e212fef9f7b9af59d63ee92e400b0f0f83cf34.png"/>
</p>
<div class="section" id="adding-an-osd">
<h3>Adding an OSD<a class="headerlink" href="#adding-an-osd" title="Permalink to this headline">¶</a></h3>
<p>Since you are running a 3-node cluster for demonstration purposes, add the OSD
to the monitor node.</p>
<div class="highlight-python"><pre>ssh node1
sudo mkdir /var/local/osd2
exit</pre>
</div>
<p>Then, from your <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> node, prepare the OSD.</p>
<div class="highlight-python"><pre>ceph-deploy osd prepare {ceph-node}:/path/to/directory</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph-deploy osd prepare node1:/var/local/osd2</pre>
</div>
<p>Finally, activate the OSDs.</p>
<div class="highlight-python"><pre>ceph-deploy osd activate {ceph-node}:/path/to/directory</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph-deploy osd activate node1:/var/local/osd2</pre>
</div>
<p>Once you have added your new OSD, Ceph will begin rebalancing the cluster by
migrating placement groups to your new OSD. You can observe this process with
the <tt class="docutils literal"><span class="pre">ceph</span></tt> CLI.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ceph</span> <span class="o">-</span><span class="n">w</span>
</pre></div>
</div>
<p>You should see the placement group states change from <tt class="docutils literal"><span class="pre">active+clean</span></tt> to active
with some degraded objects, and finally <tt class="docutils literal"><span class="pre">active+clean</span></tt> when migration
completes. (Control-c to exit.)</p>
</div>
<div class="section" id="add-a-metadata-server">
<h3>Add a Metadata Server<a class="headerlink" href="#add-a-metadata-server" title="Permalink to this headline">¶</a></h3>
<p>To use CephFS, you need at least one metadata server. Execute the following to
create a metadata server:</p>
<div class="highlight-python"><pre>ceph-deploy mds create {ceph-node}</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph-deploy mds create node1</pre>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Currently Ceph runs in production with one metadata server only. You
may use more, but there is currently no commercial support for a cluster
with multiple metadata servers.</p>
</div>
</div>
<div class="section" id="add-an-rgw-instance">
<h3>Add an RGW Instance<a class="headerlink" href="#add-an-rgw-instance" title="Permalink to this headline">¶</a></h3>
<p>To use the <a class="reference internal" href="../../../glossary/#term-ceph-object-gateway"><em class="xref std std-term">Ceph Object Gateway</em></a> component of Ceph, you must deploy an
instance of <a class="reference internal" href="../../../glossary/#term-rgw"><em class="xref std std-term">RGW</em></a>.  Execute the following to create an new instance of
RGW:</p>
<div class="highlight-python"><pre>ceph-deploy rgw create {gateway-node}</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph-deploy rgw create node1</pre>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This functionality is new with the <strong>Hammer</strong> release, and also with
<tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> v1.5.23.</p>
</div>
<p>By default, the <a class="reference internal" href="../../../glossary/#term-rgw"><em class="xref std std-term">RGW</em></a> instance will listen on port 7480. This can be
changed by editing ceph.conf on the node running the <a class="reference internal" href="../../../glossary/#term-rgw"><em class="xref std std-term">RGW</em></a> as follows:</p>
<div class="highlight-ini"><div class="highlight"><pre><span class="k">[client]</span>
<span class="na">rgw frontends</span> <span class="o">=</span> <span class="s">civetweb port=80</span>
</pre></div>
</div>
<p>To use an IPv6 address, use:</p>
<div class="highlight-ini"><div class="highlight"><pre><span class="k">[client]</span>
<span class="na">rgw frontends</span> <span class="o">=</span> <span class="s">civetweb port=[::]:80</span>
</pre></div>
</div>
</div>
<div class="section" id="adding-monitors">
<h3>Adding Monitors<a class="headerlink" href="#adding-monitors" title="Permalink to this headline">¶</a></h3>
<p>A Ceph Storage Cluster requires at least one Ceph Monitor to run. For high
availability, Ceph Storage Clusters typically run multiple Ceph
Monitors so that the failure of a single Ceph Monitor will not bring down the
Ceph Storage Cluster. Ceph uses the Paxos algorithm, which requires a majority
of monitors (i.e., 1, 2:3, 3:4, 3:5, 4:6, etc.) to form a quorum.</p>
<p>Add two Ceph Monitors to your cluster.</p>
<div class="highlight-python"><pre>ceph-deploy mon create {ceph-node}</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph-deploy mon create node2 node3</pre>
</div>
<p>Once you have added your new Ceph Monitors, Ceph will begin synchronizing
the monitors and form a quorum. You can check the quorum status by executing
the following:</p>
<div class="highlight-python"><pre>ceph quorum_status --format json-pretty</pre>
</div>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">When you run Ceph with multiple monitors, you SHOULD install and
configure NTP on each monitor host. Ensure that the
monitors are NTP peers.</p>
</div>
</div>
</div>
<div class="section" id="storing-retrieving-object-data">
<h2>Storing/Retrieving Object Data<a class="headerlink" href="#storing-retrieving-object-data" title="Permalink to this headline">¶</a></h2>
<p>To store object data in the Ceph Storage Cluster, a Ceph client must:</p>
<ol class="arabic simple">
<li>Set an object name</li>
<li>Specify a <a class="reference external" href="../../rados/operations/pools">pool</a></li>
</ol>
<p>The Ceph Client retrieves the latest cluster map and the CRUSH algorithm
calculates how to map the object to a <a class="reference external" href="../../rados/operations/placement-groups">placement group</a>, and then calculates
how to assign the placement group to a Ceph OSD Daemon dynamically. To find the
object location, all you need is the object name and the pool name. For
example:</p>
<div class="highlight-python"><pre>ceph osd map {poolname} {object-name}</pre>
</div>
<div class="topic">
<p class="topic-title first">Exercise: Locate an Object</p>
<p>As an exercise, lets create an object. Specify an object name, a path to
a test file containing some object data and a pool name using the
<tt class="docutils literal"><span class="pre">rados</span> <span class="pre">put</span></tt> command on the command line. For example:</p>
<div class="highlight-python"><pre>echo {Test-data} &gt; testfile.txt
rados put {object-name} {file-path} --pool=data
rados put test-object-1 testfile.txt --pool=data</pre>
</div>
<p>To verify that the Ceph Storage Cluster stored the object, execute
the following:</p>
<div class="highlight-python"><pre>rados -p data ls</pre>
</div>
<p>Now, identify the object location:</p>
<div class="highlight-python"><pre>ceph osd map {pool-name} {object-name}
ceph osd map data test-object-1</pre>
</div>
<p>Ceph should output the object&#8217;s location. For example:</p>
<div class="highlight-python"><pre>osdmap e537 pool 'data' (0) object 'test-object-1' -&gt; pg 0.d1743484 (0.4) -&gt; up [1,0] acting [1,0]</pre>
</div>
<p>To remove the test object, simply delete it using the <tt class="docutils literal"><span class="pre">rados</span> <span class="pre">rm</span></tt>
command.        For example:</p>
<div class="highlight-python"><pre>rados rm test-object-1 --pool=data</pre>
</div>
</div>
<p>As the cluster evolves, the object location may change dynamically. One benefit
of Ceph&#8217;s dynamic rebalancing is that Ceph relieves you from having to perform
the migration manually.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../">
              <img class="logo" src="../../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../../">Table Of Contents</a></h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../start/intro/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../start/">安装（快速）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/">安装（手动）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rbd/rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/">开发文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../release-notes/">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/">发布时间表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../glossary/">Ceph 术语</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../../../">Ceph Documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2010-2014, Inktank Storage, Inc. and contributors. Licensed under Creative Commons BY-SA.
    </div>
  </body>
</html>